[
  {
    "objectID": "slides.html#take-away-message-1",
    "href": "slides.html#take-away-message-1",
    "title": "Cataloguing and Visualizing  Big Geodata",
    "section": "Take-away message 1",
    "text": "Take-away message 1\n\nedibble is an hola experimental R-package I have been developing to plan, design, and simuate experiments"
  },
  {
    "objectID": "FinalReport.html",
    "href": "FinalReport.html",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "",
    "text": "This internship report examines the challenges and opportunities in data discovery and visualization for Satelligence, a company specializing in the use of earth observation data for detecting environmental risks and facilitate the migration towards sustainable supply chains. With the rapid growth of geospatial data, efficient access and visualization tools are crucial. In this report, the current methods employed by the company for data discovery and visualization were analyzed qualitatively, the integration of cloud-optimized data formats, cloud services, and Spatio-Temporal Asset Catalog (STAC) specifications to improve these processes were explored and the performance of dynamic tiling services in visualizing various cloud-optimized data formats was assessed. The results of the qualitative analysis showed that Satelligence’s current methods are inefficient, limit accessibility and underscore the need of a user-friendly data discovery and visualization service. Moreover, the report shows that the integration of technologies like: transitioning to cloud-optimized data formats like Cloud-Optimized Geotiffs (COGs) and Zarrs, using a dynamic STAC API standard for data organization, employing dynamic tiling libraries for optimized web visualization and deploying services using Google Kubernetes Engine was able to enhance the process of discovering and visualizing data within the company. Finally, the report suggests that dynamic tiling services perform better with COG tiles, which are 2.53 times faster to request than Zarr tiles, which aligns with COGs’ optimization for spatial data visualization. Nevertheless, new improvements and developments in the community could enhance Zarr performance in the future."
  },
  {
    "objectID": "FinalReport.html#internship-organization-background",
    "href": "FinalReport.html#internship-organization-background",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Internship organization background",
    "text": "Internship organization background\nSatelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning (Satelligence, n.d.). More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) (Satelligence, 2023). S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "FinalReport.html#context-and-justification-of-research",
    "href": "FinalReport.html#context-and-justification-of-research",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data (Satelligence, 2023). Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights (Giri and Lone, 2014). In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day (Zhao et al., 2021). This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data (Rajabifard and Williamson, 2001). However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility (Durbha et al., 2023). Due to this, an important shift is underway towards more cloud-based SDIs (Tripathi et al., 2020). These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) (Mishra and Misra, 2017). The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. with the help of Structured Query Language (SQL)) (Kaufmann and Meier, 2023). In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes (Durbha et al., 2023).\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\nCloud-optimized data formats\nCOG\nCOGs are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests (Desruisseaux et al., 2021). Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitate data interoperability in the geospatial domain (OGC, 2023).\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files (Durbha et al., 2023). Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service (Desruisseaux et al., 2021). Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC (Chester, 2024).\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data (Brodeur et al., 2019). This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) (Durbha et al., 2023).\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as applications that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data (RadiantEarth, 2024).\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other (Clark, 2020). In the case of the STAC API, it provides endpoints for searching and retrieving geo-spatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geo-spatial community. Moreover, even though STAC API is not an OGC standard or an OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning) (Holmes, 2021).\nTo facilitate easy browsing of both static and dynamic STAC catalogs, STAC Browser was created. This web-application provides a user-friendly interface to search, visualize, and, in the case of dynamic catalogs, query assets within a catalog.\n\n\nVisualization interfaces\nThe visualization of spatial data brings with it a series of challenges due to its big nature (i.e. global extent and high spatial resolution). Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory (TiTiler, n.d.). This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), COGs, and is currently working on adding support for Zarrs. For the first two, the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the visualization of multidimensional data arrays.\n\n\nCloud services\nCloud services facilitate the seamless integration of multiple services, such as the data visualization interfaces and data discovery services described before. Nevertheless, there is not a one fit all cloud solution that will always work efficiently. For instance, different cloud providers like Amazon Web Services or Google Cloud Platform (GCP) offer different tools that may offer different performances based on parameters like latency or scalability. Choosing the correct cloud service or set of services for the integration of data discovery and data visualization tools remains paramount.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geo-spatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "FinalReport.html#research-questions",
    "href": "FinalReport.html#research-questions",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow can cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications be integrated to enhance the process and experiences of discovering and visualizing big spatial data within the company?\nTo what extent do dynamic tiling services perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "FinalReport.html#sec-baseline",
    "href": "FinalReport.html#sec-baseline",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. the data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in Section 5.1 and were meant to be open questions with multiple possible answers.\nFurthermore, based on the answers of the interviewees a flowchart was built to represent visually the traditional steps performed to discover and visualize S11 data. This visual representation included estimations of the steps where more time was spent on.\nFinally, the answers to the questionnaire were analyzed qualitatively following a Thematic Content Analysis (TCA). This type of qualitative analysis focuses on finding common themes in the interviews undertaken (Anderson, 2007). The extraction of common patterns within the interviews was initially done using a large language model (i.e. Chat-GPT 3.5 (OpenAI, 2023)) using the prompt presented on Section 5.2. Moreover, the themes identified were further refined based on the interviewer’s interpretation."
  },
  {
    "objectID": "FinalReport.html#data-and-service-integration",
    "href": "FinalReport.html#data-and-service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data and service integration",
    "text": "Data and service integration\nTo efficiently integrate tools for big geo-spatial data discovery and visualization, a series of steps had to be followed. Initially, the datasets were selected. Subsequently, the structure of the catalog was defined. Following this, a Git repository containing the code required to generate the catalog was created. Static JSON files were then utilized to construct a dynamic STAC API. Ultimately, this API was deployed alongside other services using a continuous integration (CI) and continuous deployment (CD) pipeline. A further explanation of each step is presented in the following subsections.\n\nDataset selection\nDue to the desire of the company to continue moving towards a cloud-based workflow, the datasets that were considered for the catalog were composed of either COGs or Zarrs. Nevertheless, since some of the data in the company is stored as virtual rasters (VRTs), methods to also index this type of data formats in the STAC catalog were included. Specifically, S11’s long term goal is to store in the catalog datasets that can be classified as follows:\n\nStatic raster data\n\nForest baselines (Stored as COGs or Zarrs)\nThird-party data (Stored as VRTs, Tiffs, or other formats)\n\nDPROF results\n\nResults of continuous deforestation monitoring (Stored as Zarrs)\nOther DPROF results\n\nSupply chain data (Vector data)\nComplaince data (Vector data)\n\nNevertheless, the scope of this internship was limited to raster datasets. Therefore, the creation of the catalog was done using a limited amount of raster layers and they were incorporated as a proof of concept of how the catalog could be created.\n\n\nProposed Catalog structure\nThe structure of the STAC catalog proposed can be seen on Figure 1. In it, a selection of datasets that should be referenced in the catalog is presented and a hierarchical structure composed of thematic collections is suggested. This structure was not followed in the creation of the proof-of-concept catalog, as the purpose of this catalog was only to demonstrate the process of creating it. The final version of the structure will be determined by the company.\n\n\n\n\n\n\nFigure 1: Proposed STAC structure\n\n\n\n\n\nS11-cats repository\nThe s11-cats repository includes a module named cats, which consists of five sub-modules as described in Table 2. Additionally, an overview of the main workflow, which includes the creation of a new catalog each time the main function is run, is presented in Figure 2.\n\n\n\nTable 2: Description of cats submodules\n\n\n\n\n\n\n\n\n\nSubmodule\nDescription\n\n\n\n\ngcs_tools\nModule with functions to interact with data stored at Google Cloud Storage\n\n\ngeneral_metadata\nModule to extract general metadata for a STAC item.\n\n\nget_spatial_info\nModule to get all spatial information from assets.\n\n\nget_temporal_info\nModule with functions to extract temporal metadata of a dataset.\n\n\nstac_tools\nModule with the functions to initialize a STAC, add collections, items and assets to it.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: S11-cats main function\n\n\n\nAs observed, the code in the repository requires a dictionary containing collection titles, descriptions, and tags, along with a list of links for each item to be added to each collection. It then generates two JSON files: one storing the collections’ information and the other storing the items’ information. This decision to produce two JSON files was made to facilitate the transition from the static catalog that has been created to the dynamic catalog that is desired.\n\n\neoAPI + other services\nOnce a static catalog has been created, the next step involves developing the dynamic catalog by leveraging eoAPI (Sarago et al., 2024). eoAPI is a robust tool designed for managing, discovering and visualizing Earth observation data. It integrates several services that include indexing of large STAC collections and items using a Postgres database (See PgSTAC), creating a dynamic catalog that can query the Postgres database (See STAC API) and two additional services for visualizing raster (See Titiler-PgSTAC) and vector data (See TiPg).\neoAPI integrates all of these services by using containerized versions that are able to communicate seamlessly with each other. A container is a lightweight, standalone, and executable package of software that includes everything needed to run an application. Containerizing the services facilitates deployment to the cloud using Google Kubernetes Engine (GKE). Kubernetes is an open-source platform designed for automating the deployment, scaling, and management of containerized applications (Poulton, 2023). It offers various advantages, such as scalability, efficient resource utilization, and simplified maintenance, making it an ideal solution for managing the dynamic catalog and the integrated services in a cloud environment.\nSince the current version of eoAPI does not include some extra services that were necessary to deploy, a separate containerized version of these services was deployed in the same K8s cluster. Notably, a version of STAC Browser and TiTiler-Xarray to browse the catalog created and visualize Zarr datasets respectively.\n\n\nCI/CD pipeline\nFinally, a Gitlab CI/CD pipeline was created to automate the creation of the catalog using the s11-cats repository, the deployment of eoAPI and extra services and the ingestion of the catalog into the deployed version of the dynamic catalog. This step was done with the assistance of the host supervisor and consisted of the creation of automated jobs that would run every time the s11-cats repository would be updated.\n\n\nComparison with baseline scenario\nOnce a version of all of the services integrated was deployed online, the ease of discovery and visualization was again qualitatively analyzed by evaluating the steps processed for both finding and visualizing S11 data. These steps were then represented in a flowchart that could be compared to the one created in Section 2.1."
  },
  {
    "objectID": "FinalReport.html#multi-format-data-visualization",
    "href": "FinalReport.html#multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing COG and Zarr data formats, the following approach was undertaken. Firstly, a COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This pre-processing step, completed by the company prior to the study, was crucial for mimicking the COG structure, as it allowed the Zarr files to have overviews. Without these overviews, visualizing the Zarr data would require reading the data at the highest spatial resolution, significantly impacting performance. Moreover, ensuring the same data was used across both data formats enabled a direct comparison. Then, the TiTiler-Xarray service was then customized to work with the specific folder structure of the Zarr overviews previously created. Moreover, containerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were locally deployed. The performance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of twelve colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geo-spatial data visualization context.\n\nSpeed up\nThe performance of both TiTiler services to dynamically create tiles for the different data formats was evaluated using the Speed Up metric proposed in Durbha et al. (2023) (Equation 1). In this case, the Speed Up explains how much did the process of requesting tiles sped up by using a data format A compared to using a data format B.\n\\[ SpeedUp = \\frac{t_{format A}}{t_{format B}}  \\tag{1}\\]\n\n\nZoom level influence\nFinally, the effect of the level of zoom in a web map visualization on the response times of requesting tiles from the different tiling services was evaluated by fitting an Ordinary Least Squares (OLS) univariate linear regression that followed Equation 2.\n\\[ ResponseTime = \\beta_1 \\cdot ZoomLevel + \\beta_0 + \\epsilon  \\tag{2}\\]"
  },
  {
    "objectID": "FinalReport.html#baseline-scenario",
    "href": "FinalReport.html#baseline-scenario",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\n\nCurrent workflow\nOne of the main findings of the interviews was the process followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 3 and show how complex and time consuming these tasks can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying it out.\n\n\n\n\n\n\nFigure 3: Baseline workflow\n\n\n\nAccording to Figure 3, some of the most time-consuming tasks were searching for data on Google Cloud Storage and downloading it for visualization. Additionally, seeking advice from colleagues about the dataset’s location added a major uncertainty to the time estimates, as responses varied from very quick to considerably delayed or non existent.\n\n\nThematic Content Analysis\nWhen asked about the recurrent patterns on the interviews undertaken to define the baseline scenario, the large language model used in this research found four main topics:\n\nThere is a high uncertainty on the location of datasets and a high dependency on colleagues to find them.\nMultiple sources and locations of data.\nData familiarity helps users locate data more quickly.\nUse of specific tools and methods for different datasets.\n\nAfter some refinement and a deeper analysis of the interviews, the major pitfalls found on the process of data discovery and visualization in the company were summarized as follows:\n\nHigh dependency on colleagues for dataset location.\nDisorganized structure of Google Storage Buckets.\nData familiarity helps users locate data more quickly.\nLocating data is dependent on recurrent work with a specific dataset.\nNot intuitive naming of repositories with datasets.\nUnderstanding of diverse tools to access different data is currently necessary.\nDownload of data is required in most cases to visualize it.\nNot one place where all existing data can be found.\n\nAll of these pitfalls highlight the need for a simpler data discovery implementation, where data visualization can also be integrated seamlessly. Previous studies have found that key difficulties for earth observation data discovery include heterogeneous query interfaces, and use of diverse metadata models (Miranda Espinosa et al., 2020). To address these challenges, the approach should enable easy access to datasets based on specific queries, ensuring that users can efficiently locate and utilize the data they need. By harmonizing metadata standards and query protocols through the use of STAC specifications, the process of data discovery can be greatly improved, making it more accessible and user-friendly."
  },
  {
    "objectID": "FinalReport.html#service-integration",
    "href": "FinalReport.html#service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Service integration",
    "text": "Service integration\nThe integration of the services deployed resulted in a version of STAC Browser including three different collections containing datasets related to the forest baseline created by the company, elevation data from third party organizations and a collection for the comparison of COG and Zarr data. The web application can be accessed in https://eoapi.satelligence.com/browser.\n\nEffective integration\nThe effective integration was not an easy task and involved multiple aspects, ranging from editing data formats to facilitate their visualization, transitioning from a static to a dynamic catalog, customizing APIs, and finalizing with the correct deployment of the services.\n\nData formats\nAn essential step related to data formats was the edition of Zarr datasets to achieve their optimal visualization. This edition involved creating a series of overviews of the same dataset at different resolutions. Specifically, this was accomplished by converting COGs into multiple Zarr files resampled at various spatial resolutions. These resampled Zarr files, acting as overviews enhance visualization by allowing the appropriate resolution to be accessed based on the map scale, similar to the approach used when visualizing COGs (Lynnes et al., 2020). Even though the approach followed in this study allowed for improved visualization of Zarr files, the creation of Zarr pyramids in a more optimized way is still necessary. Other researchers have been focusing their efforts on this task to enhance the efficiency and effectiveness of the process (Barciauskas et al., 2024).\n\n\nLeverage of APIs\nTo effectively query the datasets stored in the catalog, a transition from a static to a dynamic catalog (i.e. a STAC API) was needed. This shift was facilitated by the deployment of a STAC API within the eoAPI framework. The STAC API facilitated the querying capabilities of the datasets stored in the catalog by dynamically requesting datasets based on their metadata. This dynamic setup not only facilitated data discovery but also enabled the use of additional tools such as the STAC API QGIS plugin. The plugin could simplify the process of data discovery and its direct manipulation.\nFor the visualization of Zarr datasets, it was necessary to customize the TiTiler-Xarray API to accommodate the new Zarr pyramid structure. This customization involved overwriting a series of functions in the main code of the application to align with the newly created Zarr pyramids. By adapting the API to handle the specific requirements of the Zarr format and its multi-resolution overviews, the visualization process was optimized.\n\n\nDeployment\nAs described on Section 2.2.4, the deployment of both eoAPI and the additional services utilized was perform using Google Kubernetes Engine (GKE), which is K8s’ GCP service. In a GKE cluster, the setup of complex multi-service applications that connect to each other with an internal network is simplified (Gupta et al., 2021). Moreover, eoAPI simplified the deployment by providing a guide for deployment that used a Helm chart. A Helm chart is a collection of files that describe the K8s related resources needed to run a multi-service application and it can improve the speed of deployment by a factor of up to 6 times (Gokhale et al., 2021). These factors greatly influenced the decision of deploying the whole suite of services in eoAPI.\nMoreover, the performance of some of the eoAPI services deployed using K8s had been already assessed by previous studies. For instance, Munteanu et al. (2024) performed tests on a deployed version of STAC API that, like the deployment performed in this study, used PgSTAC as the backend. These authors deployed a dynamic STAC API loaded with the metadata of approximately 2.3 TB of spatial data on a K8s cluster and evaluated the performance by assessing both the response times and resources used in a hypothetical scenario where 7,000 users would perform requests simultaneously. Their results showed that these services are capable of supporting effectively a much larger amount of users than the estimated by Satelligence.\nFinally, additional to the already covered advantages, the deployment of eoAPI and the fast community driven development of new tools brings with it benefits that could become very important for S11’s workflow. For instance, the visualization of vector data using TiPg is a service included in the eoAPI deployment that wasn’t used during this internship, but should certainly be integrated in the near future by the company to visualize their supply chain datasets. Moroever, the community adopting STAC specifications has been growing fast. Due to this, a big series of STAC-extensions have been developed to fulfill the requirements of the users. During this internship, extensions to add additional metadata were included, however, due to time constraints other extensions that could prove beneficial for the company were not integrated. Specifically, the possibility of adding an authentication layer to the catalog still needs to be done and should be the next step to ensure the privacy of the data.\n\n\n\nWorkflow improvement\nOnce the deployment of eoAPI and the extra services was done, a new workflow for both the new data discovery and visualization tasks was designed and is presented on Figure 4. This new workflow shows a clear improvement on the speed and the ease of use of the new methods employed.\n\n\n\n\n\n\nFigure 4: New data discovery and visualization workflow\n\n\n\nMoreover, it can be seen that with the new implementation most of the issues identified on the TCA were addressed. There is no longer a dependency on colleagues for locating datasets, as all data is now consolidated in one place. The disorganized structure of Google Storage Buckets is no longer a concern since the catalog can integrate data stored in multiple buckets into a single, cohesive STAC collection. The previous issue of non-intuitive naming conventions for data repositories, is resolved because it is unnecessary to know the data source once it is included in the STAC catalog. Furthermore, there is no longer a need to understand diverse tools for accessing different data; the STAC Browser facilitates querying collections and visualizing items. Finally, the STAC catalog serves as the centralized location for all data used in S11 workflows, which favors long term usability of code that relies on this data."
  },
  {
    "objectID": "FinalReport.html#performance-of-multi-format-data-visualization",
    "href": "FinalReport.html#performance-of-multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nThe results of the experiments made with different cloud-optimized data formats are presented in two subsections. The first subsection evaluates the overall performance of the two data formats and the second subsection assesses the performance of these data formats based on different zoom levels.\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-Xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented interactively on Figure 5 and summarized in Figure 6. In the figure it can be observed that on average the response for requests of COG tiles was 2.53 times faster than the one for the same file in ZARR format. Moreover, Figure 6 shows that the response times for tiles created from data stored as Zarr showed a considerable wider range than the ones generated from data in the COG format, which indicates more variability in the performance for Zarr.\n\nInteractive comparison\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 5: Interactive comparison of rendering speed for COG and Zarr data formats\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Response times for tile requests depending on data format\n\n\n\n\n\nThe results obtained are coherent to the ones previously shown by IMPACT (2023), where COGs’ rendering time was found to be lower than the one for Zarr files at different zoom levels. These results show that COGs, being specifically optimized for spatial data visualization, offer faster visualization compared to Zarr files. However, this does not take away from the fact that Zarr provides additional benefits, such as the ability to store n-dimensional arrays. Furthermore, recent advancements like GeoZarr and the creation of Zarr pyramids with new packages like ndpyramid could bring significant improvements to this data format Barciauskas et al. (2024).\n\n\nFine tuning of dataset\nEven though COGs showed to be very performant, their performance could be further enhanced by tuning specific GDAL parameters. These adjustments could improve the speed of tiling services using COG (IMPACT, 2023). Future considerations should include optimizing these parameters to maximize efficiency (See performance tuning section).\n\n\n\nEffects of zoom level\nAs seen on Figure 7, the zoom level of the map showed an effect on the time spent requesting and getting a tile from a tiling service for the COG format. In this study, it was found that the request times decreased by 0.01 seconds per zoom level for COGs, and didn’t show a notable change for Zarrs (+ 0.002 seconds per zoom level). The behavior presented here for COGs differs from the one observed by IMPACT (2023), where no difference in rendering time was observed as a function of the zoom level. This difference could be explained by the fact that in their study, only the lowest zoom levels were considered, while in this study only the highest zoom levels were taken into account, however, to verify this hypothesis a broader study of the response times at more zoom levels should be performed. This was not done in this study because the limited size of the study area that the raster images covered only allowed visualization of the data at high zoom levels (i.e., above 8).\n\n\n\n\n\n\n\n\nFigure 7: Request times depending on zoom level and their respective trends for COG and Zarr data formats.\n\n\n\n\n\nMoreover, as seen on Figure 8, while the size of blocks remain constant throughout all of the overviews in the COG file, the sizes of the Zarr chunks varied in the pyramids created. Due to this, the tiles requested at higher zoom levels were larger than the ones requested at lower zoom levels which could explain the difference between the trends observed in Figure 7 for the two data formats.\nFinally, despite efforts to mitigate caching effects, tiles generated from the same overview might have been reused across different zoom levels, potentially contributing to the reduced times observed for COGs at higher zoom levels. The COG used in this study had a maximum zoom level of 14. Beyond this level, tiles are generated using the same overview (i.e. the one with the highest resolution), which could indicate caching. Although both tiling services used in the study implement caching, a more in-depth investigation is needed to fully understand its impact on performance for both data formats.\n\n\n\n\n\n\n\n\nFigure 8: Variation of chunk sizes in Zarr file depending on zoom level compared to a constant block size for COG data format."
  },
  {
    "objectID": "FinalReport.html#sec-baseline-q",
    "href": "FinalReport.html#sec-baseline-q",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario questionnaire",
    "text": "Baseline scenario questionnaire\n\nRelated to data discovery\n\nI am working for Wilmar in South East Asia. Do you know what is the forest baseline that I should use and where can I find it?\nI have been checking the results of the Soy map we created. Do you know which DEM was used for it? And where can I find it?\nDo you know which DEM is used as the terrain mask when using Sentinel 1 data?\nI need to access the concessions data provided by Grepalma. Where can I find it?\n\n\n\nRelated to data visualization\n\nI am interested on getting an overview of where was the primary forest present in Colombia in 2007. Could you visualize a layer with this data for me?\nI need to visualize the concessions provided by fedepalma. Could you do it for me?"
  },
  {
    "objectID": "FinalReport.html#sec-gpt-prompt",
    "href": "FinalReport.html#sec-gpt-prompt",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Thematic Content Analysis prompt",
    "text": "Thematic Content Analysis prompt\nI will give you some notes I took from an interview I did to four study subjects:\nW, X, Y and Z.\n\nTell me if you identify any themes or topics that are repeated in the notes that \nI took from the answers of the individuals. In other words, do a simple Thematic\nContent Analysis of the interviews."
  },
  {
    "objectID": "FinalReport.html#sec-request-code",
    "href": "FinalReport.html#sec-request-code",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\nDisclaimer: In order to run the code presented below, the user must have authenticated their Google account and have the TiTiler-PgSTAC and the TiTiler-Xarray services running on localhost:8082 and localhost:8084 respectively.\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 7:\n        change_range = 1\n    elif z &lt;= 9:\n        change_range = 5\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11\n# Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\",\n        \"_name=gray&rescale=0,70\",\"_name=gray_r&rescale=0,70\",\n        \"_name=jet&rescale=0,90\", \"_name=jet_r&rescale=0,90\",\n        \"_name=hot&rescale=0,80\", \"_name=hot_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n        url_zarr = \"https://localhost:8084/tiles/WebMercatorQuad/\"+\\\n        f\"{tile}%401x?url=gs://s11-tiles/zarr/separate&\"+\\\n        \"variable=band_data&reference=false&decode_times=true&\"+\\\n        f\"consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"https://localhost:8082/collections/\"+\\\n        f\"Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/\"+\\\n        f\"WebMercatorQuad/{tile}%401x?bidx=1&assets=data&\"+\\\n        \"unscale=false&resampling=nearest&reproject=nearest&\"+\\\n        f\"colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter_k8.csv')"
  },
  {
    "objectID": "sections/appendix.html",
    "href": "sections/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "I am working for Wilmar in South East Asia. Do you know what is the forest baseline that I should use and where can I find it?\nI have been checking the results of the Soy map we created. Do you know which DEM was used for it? And where can I find it?\nDo you know which DEM is used as the terrain mask when using Sentinel 1 data?\nI need to access the concessions data provided by Grepalma. Where can I find it?\n\n\n\n\n\nI am interested on getting an overview of where was the primary forest present in Colombia in 2007. Could you visualize a layer with this data for me?\nI need to visualize the concessions provided by fedepalma. Could you do it for me?"
  },
  {
    "objectID": "sections/appendix.html#sec-baseline-q",
    "href": "sections/appendix.html#sec-baseline-q",
    "title": "Appendix",
    "section": "",
    "text": "I am working for Wilmar in South East Asia. Do you know what is the forest baseline that I should use and where can I find it?\nI have been checking the results of the Soy map we created. Do you know which DEM was used for it? And where can I find it?\nDo you know which DEM is used as the terrain mask when using Sentinel 1 data?\nI need to access the concessions data provided by Grepalma. Where can I find it?\n\n\n\n\n\nI am interested on getting an overview of where was the primary forest present in Colombia in 2007. Could you visualize a layer with this data for me?\nI need to visualize the concessions provided by fedepalma. Could you do it for me?"
  },
  {
    "objectID": "sections/appendix.html#sec-gpt-prompt",
    "href": "sections/appendix.html#sec-gpt-prompt",
    "title": "Appendix",
    "section": "Thematic Content Analysis prompt",
    "text": "Thematic Content Analysis prompt\nI will give you some notes I took from an interview I did to four study subjects:\nW, X, Y and Z.\n\nTell me if you identify any themes or topics that are repeated in the notes that \nI took from the answers of the individuals. In other words, do a simple Thematic\nContent Analysis of the interviews."
  },
  {
    "objectID": "sections/appendix.html#sec-request-code",
    "href": "sections/appendix.html#sec-request-code",
    "title": "Appendix",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\nDisclaimer: In order to run the code presented below, the user must have authenticated their Google account and have the TiTiler-PgSTAC and the TiTiler-Xarray services running on localhost:8082 and localhost:8084 respectively.\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 7:\n        change_range = 1\n    elif z &lt;= 9:\n        change_range = 5\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11\n# Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\",\n        \"_name=gray&rescale=0,70\",\"_name=gray_r&rescale=0,70\",\n        \"_name=jet&rescale=0,90\", \"_name=jet_r&rescale=0,90\",\n        \"_name=hot&rescale=0,80\", \"_name=hot_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n        url_zarr = \"https://localhost:8084/tiles/WebMercatorQuad/\"+\\\n        f\"{tile}%401x?url=gs://s11-tiles/zarr/separate&\"+\\\n        \"variable=band_data&reference=false&decode_times=true&\"+\\\n        f\"consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"https://localhost:8082/collections/\"+\\\n        f\"Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/\"+\\\n        f\"WebMercatorQuad/{tile}%401x?bidx=1&assets=data&\"+\\\n        \"unscale=false&resampling=nearest&reproject=nearest&\"+\\\n        f\"colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter_k8.csv')"
  },
  {
    "objectID": "sections/results.html",
    "href": "sections/results.html",
    "title": "Baseline scenario",
    "section": "",
    "text": "One of the main findings of the interviews was the process followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 1 and show how complex and time consuming these tasks can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying it out.\n\n\n\n\n\n\nFigure 1: Baseline workflow\n\n\n\nAccording to Figure 1, some of the most time-consuming tasks were searching for data on Google Cloud Storage and downloading it for visualization. Additionally, seeking advice from colleagues about the dataset’s location added a major uncertainty to the time estimates, as responses varied from very quick to considerably delayed or non existent."
  },
  {
    "objectID": "sections/results.html#service-integration",
    "href": "sections/results.html#service-integration",
    "title": "Baseline scenario",
    "section": "Service integration",
    "text": "Service integration\nThe integration of the services deployed resulted in a version of STAC Browser including three different collections containing datasets related to the forest baseline created by the company, elevation data from third party organizations and a collection for the comparison of COG and Zarr data. The web application can be accessed in https://eoapi.satelligence.com/browser.\n\nEffective integration\nThe effective integration was not an easy task and involved multiple aspects, ranging from editing data formats to facilitate their visualization, transitioning from a static to a dynamic catalog, customizing APIs, and finalizing with the correct deployment of the services.\n\nData formats\nAn essential step related to data formats was the edition of Zarr datasets to achieve their optimal visualization. This edition involved creating a series of overviews of the same dataset at different resolutions. Specifically, this was accomplished by converting COGs into multiple Zarr files resampled at various spatial resolutions. These resampled Zarr files, acting as overviews enhance visualization by allowing the appropriate resolution to be accessed based on the map scale, similar to the approach used when visualizing COGs [@lynnes_cloud_2020]. Even though the approach followed in this study allowed for improved visualization of Zarr files, the creation of Zarr pyramids in a more optimized way is still necessary. Other researchers have been focusing their efforts on this task to enhance the efficiency and effectiveness of the process [@Barciauskas_NextGen_2024].\n\n\nLeverage of APIs\nTo effectively query the datasets stored in the catalog, a transition from a static to a dynamic catalog (i.e. a STAC API) was needed. This shift was facilitated by the deployment of a STAC API within the eoAPI framework. The STAC API facilitated the querying capabilities of the datasets stored in the catalog by dynamically requesting datasets based on their metadata. This dynamic setup not only facilitated data discovery but also enabled the use of additional tools such as the STAC API QGIS plugin. The plugin could simplify the process of data discovery and its direct manipulation.\nFor the visualization of Zarr datasets, it was necessary to customize the TiTiler-Xarray API to accommodate the new Zarr pyramid structure. This customization involved overwriting a series of functions in the main code of the application to align with the newly created Zarr pyramids. By adapting the API to handle the specific requirements of the Zarr format and its multi-resolution overviews, the visualization process was optimized.\n\n\nDeployment\nAs described on ?@sec-eoapi, the deployment of both eoAPI and the additional services utilized was perform using Google Kubernetes Engine (GKE), which is K8s’ GCP service. In a GKE cluster, the setup of complex multi-service applications that connect to each other with an internal network is simplified [@gupta_deployment_2021]. Moreover, eoAPI simplified the deployment by providing a guide for deployment that used a Helm chart. A Helm chart is a collection of files that describe the K8s related resources needed to run a multi-service application and it can improve the speed of deployment by a factor of up to 6 times [@gokhale_creating_2021]. These factors greatly influenced the decision of deploying the whole suite of services in eoAPI.\nMoreover, the performance of some of the eoAPI services deployed using K8s had been already assessed by previous studies. For instance, @munteanu_performance_2024 performed tests on a deployed version of STAC API that, like the deployment performed in this study, used PgSTAC as the backend. These authors deployed a dynamic STAC API loaded with the metadata of approximately 2.3 TB of spatial data on a K8s cluster and evaluated the performance by assessing both the response times and resources used in a hypothetical scenario where 7,000 users would perform requests simultaneously. Their results showed that these services are capable of supporting effectively a much larger amount of users than the estimated by Satelligence.\nFinally, additional to the already covered advantages, the deployment of eoAPI and the fast community driven development of new tools brings with it benefits that could become very important for S11’s workflow. For instance, the visualization of vector data using TiPg is a service included in the eoAPI deployment that wasn’t used during this internship, but should certainly be integrated in the near future by the company to visualize their supply chain datasets. Moroever, the community adopting STAC specifications has been growing fast. Due to this, a big series of STAC-extensions have been developed to fulfill the requirements of the users. During this internship, extensions to add additional metadata were included, however, due to time constraints other extensions that could prove beneficial for the company were not integrated. Specifically, the possibility of adding an authentication layer to the catalog still needs to be done and should be the next step to ensure the privacy of the data.\n\n\n\nWorkflow improvement\nOnce the deployment of eoAPI and the extra services was done, a new workflow for both the new data discovery and visualization tasks was designed and is presented on Figure 2. This new workflow shows a clear improvement on the speed and the ease of use of the new methods employed.\n\n\n\n\n\n\nFigure 2: New data discovery and visualization workflow\n\n\n\nMoreover, it can be seen that with the new implementation most of the issues identified on the TCA were addressed. There is no longer a dependency on colleagues for locating datasets, as all data is now consolidated in one place. The disorganized structure of Google Storage Buckets is no longer a concern since the catalog can integrate data stored in multiple buckets into a single, cohesive STAC collection. The previous issue of non-intuitive naming conventions for data repositories, is resolved because it is unnecessary to know the data source once it is included in the STAC catalog. Furthermore, there is no longer a need to understand diverse tools for accessing different data; the STAC Browser facilitates querying collections and visualizing items. Finally, the STAC catalog serves as the centralized location for all data used in S11 workflows, which favors long term usability of code that relies on this data."
  },
  {
    "objectID": "sections/results.html#performance-of-multi-format-data-visualization",
    "href": "sections/results.html#performance-of-multi-format-data-visualization",
    "title": "Baseline scenario",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nThe results of the experiments made with different cloud-optimized data formats are presented in two subsections. The first subsection evaluates the overall performance of the two data formats and the second subsection assesses the performance of these data formats based on different zoom levels.\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-Xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented interactively on Figure 3 and summarized in Figure 4. In the figure it can be observed that on average the response for requests of COG tiles was 2.53 times faster than the one for the same file in ZARR format. Moreover, Figure 4 shows that the response times for tiles created from data stored as Zarr showed a considerable wider range than the ones generated from data in the COG format, which indicates more variability in the performance for Zarr.\n\nInteractive comparison\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 3: Interactive comparison of rendering speed for COG and Zarr data formats\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Response times for tile requests depending on data format\n\n\n\n\n\nThe results obtained are coherent to the ones previously shown by @nasa_impact_zarr_2023, where COGs’ rendering time was found to be lower than the one for Zarr files at different zoom levels. These results show that COGs, being specifically optimized for spatial data visualization, offer faster visualization compared to Zarr files. However, this does not take away from the fact that Zarr provides additional benefits, such as the ability to store n-dimensional arrays. Furthermore, recent advancements like GeoZarr and the creation of Zarr pyramids with new packages like ndpyramid could bring significant improvements to this data format [@pagan_current_2023, @Barciauskas_NextGen_2024].\n\n\nFine tuning of dataset\nEven though COGs showed to be very performant, their performance could be further enhanced by tuning specific GDAL parameters. These adjustments could improve the speed of tiling services using COG [@nasa_impact_zarr_2023]. Future considerations should include optimizing these parameters to maximize efficiency (See performance tuning section).\n\n\n\nEffects of zoom level\nAs seen on Figure 5, the zoom level of the map showed an effect on the time spent requesting and getting a tile from a tiling service for the COG format. In this study, it was found that the request times decreased by 0.01 seconds per zoom level for COGs, and didn’t show a notable change for Zarrs (+ 0.002 seconds per zoom level). The behavior presented here for COGs differs from the one observed by @nasa_impact_zarr_2023, where no difference in rendering time was observed as a function of the zoom level. This difference could be explained by the fact that in their study, only the lowest zoom levels were considered, while in this study only the highest zoom levels were taken into account, however, to verify this hypothesis a broader study of the response times at more zoom levels should be performed. This was not done in this study because the limited size of the study area that the raster images covered only allowed visualization of the data at high zoom levels (i.e., above 8).\n\n\n\n\n\n\n\n\nFigure 5: Request times depending on zoom level and their respective trends for COG and Zarr data formats.\n\n\n\n\n\nMoreover, as seen on Figure 6, while the size of blocks remain constant throughout all of the overviews in the COG file, the sizes of the Zarr chunks varied in the pyramids created. Due to this, the tiles requested at higher zoom levels were larger than the ones requested at lower zoom levels which could explain the difference between the trends observed in Figure 5 for the two data formats.\nFinally, despite efforts to mitigate caching effects, tiles generated from the same overview might have been reused across different zoom levels, potentially contributing to the reduced times observed for COGs at higher zoom levels. The COG used in this study had a maximum zoom level of 14. Beyond this level, tiles are generated using the same overview (i.e. the one with the highest resolution), which could indicate caching. Although both tiling services used in the study implement caching, a more in-depth investigation is needed to fully understand its impact on performance for both data formats.\n\n\n\n\n\n\n\n\nFigure 6: Variation of chunk sizes in Zarr file depending on zoom level compared to a constant block size for COG data format."
  },
  {
    "objectID": "sections/introduction.html",
    "href": "sections/introduction.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "Satelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) [@satelligence_internship_2023]. S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "sections/introduction.html#internship-organization-background",
    "href": "sections/introduction.html#internship-organization-background",
    "title": "S11-CATS Report",
    "section": "",
    "text": "Satelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) [@satelligence_internship_2023]. S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "sections/introduction.html#context-and-justification-of-research",
    "href": "sections/introduction.html#context-and-justification-of-research",
    "title": "S11-CATS Report",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data [@satelligence_internship_2023]. Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights [@giri_big_2014]. In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day [@zhao_scalable_2021]. This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data [@rajabifard_spatial_2001]. However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility [@durbha_advances_2023]. Due to this, an important shift is underway towards more cloud-based SDIs [@tripathi_cloud_2020]. These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) [@mishra_structured_2017]. The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. with the help of Structured Query Language (SQL)) [@kaufmann_database_2023]. In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes [@durbha_advances_2023].\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\nCloud-optimized data formats\nCOG\nCOGs are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests [@desruisseaux_ogc_2021]. Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitate data interoperability in the geospatial domain [@ogc_ogc_2023].\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files [@durbha_advances_2023]. Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service [@desruisseaux_ogc_2021]. Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC [@chester_ogc_2024].\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data [@brodeur_geographic_2019]. This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) [@durbha_advances_2023].\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as applications that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data [@noauthor_stac-specbest-practicesmd_nodate].\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other [@clark_api_2020]. In the case of the STAC API, it provides endpoints for searching and retrieving geo-spatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geo-spatial community. Moreover, even though STAC API is not an OGC standard or an OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning) [@holmes_spatiotemporal_2021].\nTo facilitate easy browsing of both static and dynamic STAC catalogs, STAC Browser was created. This web-application provides a user-friendly interface to search, visualize, and, in the case of dynamic catalogs, query assets within a catalog.\n\n\nVisualization interfaces\nThe visualization of spatial data brings with it a series of challenges due to its big nature (i.e. global extent and high spatial resolution). Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory [@noauthor_titiler_nodate]. This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), COGs, and is currently working on adding support for Zarrs. For the first two, the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the visualization of multidimensional data arrays.\n\n\nCloud services\nCloud services facilitate the seamless integration of multiple services, such as the data visualization interfaces and data discovery services described before. Nevertheless, there is not a one fit all cloud solution that will always work efficiently. For instance, different cloud providers like Amazon Web Services or Google Cloud Platform (GCP) offer different tools that may offer different performances based on parameters like latency or scalability. Choosing the correct cloud service or set of services for the integration of data discovery and data visualization tools remains paramount.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geo-spatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "sections/introduction.html#research-questions",
    "href": "sections/introduction.html#research-questions",
    "title": "S11-CATS Report",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow can cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications be integrated to enhance the process and experiences of discovering and visualizing big spatial data within the company?\nTo what extent do dynamic tiling services perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "sections/abstract.html",
    "href": "sections/abstract.html",
    "title": "Executive summary",
    "section": "",
    "text": "Executive summary\nThis internship report examines the challenges and opportunities in data discovery and visualization for Satelligence, a company specializing in the use of earth observation data for detecting environmental risks and facilitate the migration towards sustainable supply chains. With the rapid growth of geospatial data, efficient access and visualization tools are crucial. In this report, the current methods employed by the company for data discovery and visualization were analyzed qualitatively, the integration of cloud-optimized data formats, cloud services, and Spatio-Temporal Asset Catalog (STAC) specifications to improve these processes were explored and the performance of dynamic tiling services in visualizing various cloud-optimized data formats was assessed. The results of the qualitative analysis showed that Satelligence’s current methods are inefficient, limit accessibility and underscore the need of a user-friendly data discovery and visualization service. Moreover, the report shows that the integration of technologies like: transitioning to cloud-optimized data formats like Cloud-Optimized Geotiffs (COGs) and Zarrs, using a dynamic STAC API standard for data organization, employing dynamic tiling libraries for optimized web visualization and deploying services using Google Kubernetes Engine was able to enhance the process of discovering and visualizing data within the company. Finally, the report suggests that dynamic tiling services perform better with COG tiles, which are 2.53 times faster to request than Zarr tiles, which aligns with COGs’ optimization for spatial data visualization. Nevertheless, new improvements and developments in the community could enhance Zarr performance in the future.\n\n\nList of abbreviations\n\nAbbreviation list\n\n\nAbreviations\nDescription\n\n\n\n\nEUDR\nEuropean Union Deforestation Regulation\n\n\nSTAC\nSpatio-Temporal Asset Catalog\n\n\nCOG\nCloud-Optimized GeoTiff\n\n\nOGC\nOpen Geospatial Consortium\n\n\nSDI\nSpatial Data Infrastructure\n\n\nGCP\nGoogle Cloud Platform\n\n\nS11\nSatelligence\n\n\nK8s\nKubernetes\n\n\nDPROF\nDistributed Processing Framework\n\n\nJSON\nJavaScript Object Notation\n\n\nAPI\nApplication Programming Interfaces\n\n\nHTTP\nHyperText Transfer Protocol\n\n\nSQL\nStandard Query Language\n\n\nFBL\nForest Baseline\n\n\nDEM\nDigital Elevation Map\n\n\nTCA\nThematic Content analysis\n\n\nVRT\nVitual Raster"
  },
  {
    "objectID": "sections/conclusions.html",
    "href": "sections/conclusions.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "The conclusions of the work performed during this internship are presented below in relationship to each research question:\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\n\nThe qualitative analysis of the current workflow at Satelligence revealed a complex and time-consuming process for data discovery, retrieval, and visualization. Major inefficiencies identified by the TCA included high dependency on colleagues for locating datasets, a disorganized structure of Google Storage Buckets and the need to download full datasets for visualization. The challenges found underscored the necessity for a user-friendly data discovery implementation where data visualization could also be integrated.\n\nHow can cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications be integrated to enhance the process and experiences of discovering and visualizing big spatial data within the company?\n\nThe integration of cloud-optimized data formats, cloud services, and SpatioTemporal Asset Catalog (STAC) specifications has greatly enhanced data discovery and visualization for Satelligence. Despite the challenges that could be involved in integrating these aspects, this study presents a methodology that leverages customized cloud optimized data formats, the integration of several services (i.e. eoAPI, STAC Browser and TiTiler-Xarray), and the deployment using GKE to achieve effective integration. The solution proposed here addresses challenges found previously. Finally, even though the consolidation of all data in one place ensures long-term usability and an easier management of big spatial data, future steps should prioritize the createion of a more optimized set of Zarr pyramids and the addition of an authentication layer to ensure the data privacy.\n\nTo what extent do dynamic tiling services perform in visualizing different cloud-optimized data formats?\n\nDynamic tiling services vary in performance when visualizing different cloud-optimized data-formats. In this study, the performance evaluation showed that COG tiles were, on average, 2.53 times faster to be requested than Zarr tiles, with less variability in response times. These results align with the understanding that COGs are specifically optimized for spatial data visualization, whereas Zarrs are not. Although this study made efforts to improve Zarr visualization using customized pyramids, recent advancements such as GeoZarr and more efficient Zarr pyramids generation could further enhance Zarr visualization performance. Finally, while the effect of caching in tiling services was not studied in this internship, it can have a considerable impact on response times and should be considered in future research."
  },
  {
    "objectID": "sections/methodology.html",
    "href": "sections/methodology.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "To answer the research questions presented, a series of tasks were undertaken. These tasks are presented in the following subsections where they are divided by research question."
  },
  {
    "objectID": "sections/methodology.html#sec-baseline",
    "href": "sections/methodology.html#sec-baseline",
    "title": "S11-CATS Report",
    "section": "Baseline scenario",
    "text": "Baseline scenario\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. the data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in ?@sec-baseline-q and were meant to be open questions with multiple possible answers.\nFurthermore, based on the answers of the interviewees a flowchart was built to represent visually the traditional steps performed to discover and visualize S11 data. This visual representation included estimations of the steps where more time was spent on.\nFinally, the answers to the questionnaire were analyzed qualitatively following a Thematic Content Analysis (TCA). This type of qualitative analysis focuses on finding common themes in the interviews undertaken [@anderson_thematic_2007]. The extraction of common patterns within the interviews was initially done using a large language model (i.e. Chat-GPT 3.5 [@openai_chatgpt_2023]) using the prompt presented on ?@sec-gpt-prompt. Moreover, the themes identified were further refined based on the interviewer’s interpretation."
  },
  {
    "objectID": "sections/methodology.html#data-and-service-integration",
    "href": "sections/methodology.html#data-and-service-integration",
    "title": "S11-CATS Report",
    "section": "Data and service integration",
    "text": "Data and service integration\nTo efficiently integrate tools for big geo-spatial data discovery and visualization, a series of steps had to be followed. Initially, the datasets were selected. Subsequently, the structure of the catalog was defined. Following this, a Git repository containing the code required to generate the catalog was created. Static JSON files were then utilized to construct a dynamic STAC API. Ultimately, this API was deployed alongside other services using a continuous integration (CI) and continuous deployment (CD) pipeline. A further explanation of each step is presented in the following subsections.\n\nDataset selection\nDue to the desire of the company to continue moving towards a cloud-based workflow, the datasets that were considered for the catalog were composed of either COGs or Zarrs. Nevertheless, since some of the data in the company is stored as virtual rasters (VRTs), methods to also index this type of data formats in the STAC catalog were included. Specifically, S11’s long term goal is to store in the catalog datasets that can be classified as follows:\n\nStatic raster data\n\nForest baselines (Stored as COGs or Zarrs)\nThird-party data (Stored as VRTs, Tiffs, or other formats)\n\nDPROF results\n\nResults of continuous deforestation monitoring (Stored as Zarrs)\nOther DPROF results\n\nSupply chain data (Vector data)\nComplaince data (Vector data)\n\nNevertheless, the scope of this internship was limited to raster datasets. Therefore, the creation of the catalog was done using a limited amount of raster layers and they were incorporated as a proof of concept of how the catalog could be created.\n\n\nProposed Catalog structure\nThe structure of the STAC catalog proposed can be seen on Figure 1. In it, a selection of datasets that should be referenced in the catalog is presented and a hierarchical structure composed of thematic collections is suggested. This structure was not followed in the creation of the proof-of-concept catalog, as the purpose of this catalog was only to demonstrate the process of creating it. The final version of the structure will be determined by the company.\n\n\n\n\n\n\nFigure 1: Proposed STAC structure\n\n\n\n\n\nS11-cats repository\nThe s11-cats repository includes a module named cats, which consists of five sub-modules as described in Table 1. Additionally, an overview of the main workflow, which includes the creation of a new catalog each time the main function is run, is presented in Figure 2.\n\n\n\nTable 1: Description of cats submodules\n\n\n\n\n\n\n\n\n\nSubmodule\nDescription\n\n\n\n\ngcs_tools\nModule with functions to interact with data stored at Google Cloud Storage\n\n\ngeneral_metadata\nModule to extract general metadata for a STAC item.\n\n\nget_spatial_info\nModule to get all spatial information from assets.\n\n\nget_temporal_info\nModule with functions to extract temporal metadata of a dataset.\n\n\nstac_tools\nModule with the functions to initialize a STAC, add collections, items and assets to it.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: S11-cats main function\n\n\n\nAs observed, the code in the repository requires a dictionary containing collection titles, descriptions, and tags, along with a list of links for each item to be added to each collection. It then generates two JSON files: one storing the collections’ information and the other storing the items’ information. This decision to produce two JSON files was made to facilitate the transition from the static catalog that has been created to the dynamic catalog that is desired.\n\n\neoAPI + other services\nOnce a static catalog has been created, the next step involves developing the dynamic catalog by leveraging eoAPI [@sarago_developmentseedeoapi_2024]. eoAPI is a robust tool designed for managing, discovering and visualizing Earth observation data. It integrates several services that include indexing of large STAC collections and items using a Postgres database (See PgSTAC), creating a dynamic catalog that can query the Postgres database (See STAC API) and two additional services for visualizing raster (See Titiler-PgSTAC) and vector data (See TiPg).\neoAPI integrates all of these services by using containerized versions that are able to communicate seamlessly with each other. A container is a lightweight, standalone, and executable package of software that includes everything needed to run an application. Containerizing the services facilitates deployment to the cloud using Google Kubernetes Engine (GKE). Kubernetes is an open-source platform designed for automating the deployment, scaling, and management of containerized applications [@poulton_kubernetes_2023]. It offers various advantages, such as scalability, efficient resource utilization, and simplified maintenance, making it an ideal solution for managing the dynamic catalog and the integrated services in a cloud environment.\nSince the current version of eoAPI does not include some extra services that were necessary to deploy, a separate containerized version of these services was deployed in the same K8s cluster. Notably, a version of STAC Browser and TiTiler-Xarray to browse the catalog created and visualize Zarr datasets respectively.\n\n\nCI/CD pipeline\nFinally, a Gitlab CI/CD pipeline was created to automate the creation of the catalog using the s11-cats repository, the deployment of eoAPI and extra services and the ingestion of the catalog into the deployed version of the dynamic catalog. This step was done with the assistance of the host supervisor and consisted of the creation of automated jobs that would run every time the s11-cats repository would be updated.\n\n\nComparison with baseline scenario\nOnce a version of all of the services integrated was deployed online, the ease of discovery and visualization was again qualitatively analyzed by evaluating the steps processed for both finding and visualizing S11 data. These steps were then represented in a flowchart that could be compared to the one created in Section 1."
  },
  {
    "objectID": "sections/methodology.html#multi-format-data-visualization",
    "href": "sections/methodology.html#multi-format-data-visualization",
    "title": "S11-CATS Report",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing COG and Zarr data formats, the following approach was undertaken. Firstly, a COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This pre-processing step, completed by the company prior to the study, was crucial for mimicking the COG structure, as it allowed the Zarr files to have overviews. Without these overviews, visualizing the Zarr data would require reading the data at the highest spatial resolution, significantly impacting performance. Moreover, ensuring the same data was used across both data formats enabled a direct comparison. Then, the TiTiler-Xarray service was then customized to work with the specific folder structure of the Zarr overviews previously created. Moreover, containerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were locally deployed. The performance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of twelve colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geo-spatial data visualization context.\n\nSpeed up\nThe performance of both TiTiler services to dynamically create tiles for the different data formats was evaluated using the Speed Up metric proposed in @durbha_advances_2023 (Equation 1). In this case, the Speed Up explains how much did the process of requesting tiles sped up by using a data format A compared to using a data format B.\n\\[ SpeedUp = \\frac{t_{format A}}{t_{format B}}  \\tag{1}\\]\n\n\nZoom level influence\nFinally, the effect of the level of zoom in a web map visualization on the response times of requesting tiles from the different tiling services was evaluated by fitting an Ordinary Least Squares (OLS) univariate linear regression that followed Equation 2.\n\\[ ResponseTime = \\beta_1 \\cdot ZoomLevel + \\beta_0 + \\epsilon  \\tag{2}\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "",
    "text": "This internship report examines the challenges and opportunities in data discovery and visualization for Satelligence, a company specializing in the use of earth observation data for detecting environmental risks and facilitate the migration towards sustainable supply chains. With the rapid growth of geospatial data, efficient access and visualization tools are crucial. In this report, the current methods employed by the company for data discovery and visualization were analyzed qualitatively, the integration of cloud-optimized data formats, cloud services, and Spatio-Temporal Asset Catalog (STAC) specifications to improve these processes were explored and the performance of dynamic tiling services in visualizing various cloud-optimized data formats was assessed. The results of the qualitative analysis showed that Satelligence’s current methods are inefficient, limit accessibility and underscore the need of a user-friendly data discovery and visualization service. Moreover, the report shows that the integration of technologies like: transitioning to cloud-optimized data formats like Cloud-Optimized Geotiffs (COGs) and Zarrs, using a dynamic STAC API standard for data organization, employing dynamic tiling libraries for optimized web visualization and deploying services using Google Kubernetes Engine was able to enhance the process of discovering and visualizing data within the company. Finally, the report suggests that dynamic tiling services perform better with COG tiles, which are 2.53 times faster to request than Zarr tiles, which aligns with COGs’ optimization for spatial data visualization. Nevertheless, new improvements and developments in the community could enhance Zarr performance in the future."
  },
  {
    "objectID": "index.html#internship-organization-background",
    "href": "index.html#internship-organization-background",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Internship organization background",
    "text": "Internship organization background\nSatelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning (Satelligence, n.d.). More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) (Satelligence, 2023). S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "index.html#context-and-justification-of-research",
    "href": "index.html#context-and-justification-of-research",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data (Satelligence, 2023). Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights (Giri and Lone, 2014). In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day (Zhao et al., 2021). This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data (Rajabifard and Williamson, 2001). However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility (Durbha et al., 2023). Due to this, an important shift is underway towards more cloud-based SDIs (Tripathi et al., 2020). These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) (Mishra and Misra, 2017). The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. with the help of Structured Query Language (SQL)) (Kaufmann and Meier, 2023). In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes (Durbha et al., 2023).\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\nCloud-optimized data formats\nCOG\nCOGs are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests (Desruisseaux et al., 2021). Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitate data interoperability in the geospatial domain (OGC, 2023).\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files (Durbha et al., 2023). Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service (Desruisseaux et al., 2021). Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC (Chester, 2024).\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data (Brodeur et al., 2019). This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) (Durbha et al., 2023).\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as applications that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data (RadiantEarth, 2024).\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other (Clark, 2020). In the case of the STAC API, it provides endpoints for searching and retrieving geo-spatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geo-spatial community. Moreover, even though STAC API is not an OGC standard or an OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning) (Holmes, 2021).\nTo facilitate easy browsing of both static and dynamic STAC catalogs, STAC Browser was created. This web-application provides a user-friendly interface to search, visualize, and, in the case of dynamic catalogs, query assets within a catalog.\n\n\nVisualization interfaces\nThe visualization of spatial data brings with it a series of challenges due to its big nature (i.e. global extent and high spatial resolution). Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory (TiTiler, n.d.). This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), COGs, and is currently working on adding support for Zarrs. For the first two, the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the visualization of multidimensional data arrays.\n\n\nCloud services\nCloud services facilitate the seamless integration of multiple services, such as the data visualization interfaces and data discovery services described before. Nevertheless, there is not a one fit all cloud solution that will always work efficiently. For instance, different cloud providers like Amazon Web Services or Google Cloud Platform (GCP) offer different tools that may offer different performances based on parameters like latency or scalability. Choosing the correct cloud service or set of services for the integration of data discovery and data visualization tools remains paramount.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geo-spatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow can cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications be integrated to enhance the process and experiences of discovering and visualizing big spatial data within the company?\nTo what extent do dynamic tiling services perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "index.html#sec-baseline",
    "href": "index.html#sec-baseline",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. the data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in Section 5.1 and were meant to be open questions with multiple possible answers.\nFurthermore, based on the answers of the interviewees a flowchart was built to represent visually the traditional steps performed to discover and visualize S11 data. This visual representation included estimations of the steps where more time was spent on.\nFinally, the answers to the questionnaire were analyzed qualitatively following a Thematic Content Analysis (TCA). This type of qualitative analysis focuses on finding common themes in the interviews undertaken (Anderson, 2007). The extraction of common patterns within the interviews was initially done using a large language model (i.e. Chat-GPT 3.5 (OpenAI, 2023)) using the prompt presented on Section 5.2. Moreover, the themes identified were further refined based on the interviewer’s interpretation."
  },
  {
    "objectID": "index.html#data-and-service-integration",
    "href": "index.html#data-and-service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data and service integration",
    "text": "Data and service integration\nTo efficiently integrate tools for big geo-spatial data discovery and visualization, a series of steps had to be followed. Initially, the datasets were selected. Subsequently, the structure of the catalog was defined. Following this, a Git repository containing the code required to generate the catalog was created. Static JSON files were then utilized to construct a dynamic STAC API. Ultimately, this API was deployed alongside other services using a continuous integration (CI) and continuous deployment (CD) pipeline. A further explanation of each step is presented in the following subsections.\n\nDataset selection\nDue to the desire of the company to continue moving towards a cloud-based workflow, the datasets that were considered for the catalog were composed of either COGs or Zarrs. Nevertheless, since some of the data in the company is stored as virtual rasters (VRTs), methods to also index this type of data formats in the STAC catalog were included. Specifically, S11’s long term goal is to store in the catalog datasets that can be classified as follows:\n\nStatic raster data\n\nForest baselines (Stored as COGs or Zarrs)\nThird-party data (Stored as VRTs, Tiffs, or other formats)\n\nDPROF results\n\nResults of continuous deforestation monitoring (Stored as Zarrs)\nOther DPROF results\n\nSupply chain data (Vector data)\nComplaince data (Vector data)\n\nNevertheless, the scope of this internship was limited to raster datasets. Therefore, the creation of the catalog was done using a limited amount of raster layers and they were incorporated as a proof of concept of how the catalog could be created.\n\n\nProposed Catalog structure\nThe structure of the STAC catalog proposed can be seen on Figure 1. In it, a selection of datasets that should be referenced in the catalog is presented and a hierarchical structure composed of thematic collections is suggested. This structure was not followed in the creation of the proof-of-concept catalog, as the purpose of this catalog was only to demonstrate the process of creating it. The final version of the structure will be determined by the company.\n\n\n\n\n\n\nFigure 1: Proposed STAC structure\n\n\n\n\n\nS11-cats repository\nThe s11-cats repository includes a module named cats, which consists of five sub-modules as described in Table 2. Additionally, an overview of the main workflow, which includes the creation of a new catalog each time the main function is run, is presented in Figure 2.\n\n\n\nTable 2: Description of cats submodules\n\n\n\n\n\n\n\n\n\nSubmodule\nDescription\n\n\n\n\ngcs_tools\nModule with functions to interact with data stored at Google Cloud Storage\n\n\ngeneral_metadata\nModule to extract general metadata for a STAC item.\n\n\nget_spatial_info\nModule to get all spatial information from assets.\n\n\nget_temporal_info\nModule with functions to extract temporal metadata of a dataset.\n\n\nstac_tools\nModule with the functions to initialize a STAC, add collections, items and assets to it.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: S11-cats main function\n\n\n\nAs observed, the code in the repository requires a dictionary containing collection titles, descriptions, and tags, along with a list of links for each item to be added to each collection. It then generates two JSON files: one storing the collections’ information and the other storing the items’ information. This decision to produce two JSON files was made to facilitate the transition from the static catalog that has been created to the dynamic catalog that is desired.\n\n\neoAPI + other services\nOnce a static catalog has been created, the next step involves developing the dynamic catalog by leveraging eoAPI (Sarago et al., 2024). eoAPI is a robust tool designed for managing, discovering and visualizing Earth observation data. It integrates several services that include indexing of large STAC collections and items using a Postgres database (See PgSTAC), creating a dynamic catalog that can query the Postgres database (See STAC API) and two additional services for visualizing raster (See Titiler-PgSTAC) and vector data (See TiPg).\neoAPI integrates all of these services by using containerized versions that are able to communicate seamlessly with each other. A container is a lightweight, standalone, and executable package of software that includes everything needed to run an application. Containerizing the services facilitates deployment to the cloud using Google Kubernetes Engine (GKE). Kubernetes is an open-source platform designed for automating the deployment, scaling, and management of containerized applications (Poulton, 2023). It offers various advantages, such as scalability, efficient resource utilization, and simplified maintenance, making it an ideal solution for managing the dynamic catalog and the integrated services in a cloud environment.\nSince the current version of eoAPI does not include some extra services that were necessary to deploy, a separate containerized version of these services was deployed in the same K8s cluster. Notably, a version of STAC Browser and TiTiler-Xarray to browse the catalog created and visualize Zarr datasets respectively.\n\n\nCI/CD pipeline\nFinally, a Gitlab CI/CD pipeline was created to automate the creation of the catalog using the s11-cats repository, the deployment of eoAPI and extra services and the ingestion of the catalog into the deployed version of the dynamic catalog. This step was done with the assistance of the host supervisor and consisted of the creation of automated jobs that would run every time the s11-cats repository would be updated.\n\n\nComparison with baseline scenario\nOnce a version of all of the services integrated was deployed online, the ease of discovery and visualization was again qualitatively analyzed by evaluating the steps processed for both finding and visualizing S11 data. These steps were then represented in a flowchart that could be compared to the one created in Section 2.1."
  },
  {
    "objectID": "index.html#multi-format-data-visualization",
    "href": "index.html#multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing COG and Zarr data formats, the following approach was undertaken. Firstly, a COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This pre-processing step, completed by the company prior to the study, was crucial for mimicking the COG structure, as it allowed the Zarr files to have overviews. Without these overviews, visualizing the Zarr data would require reading the data at the highest spatial resolution, significantly impacting performance. Moreover, ensuring the same data was used across both data formats enabled a direct comparison. Then, the TiTiler-Xarray service was then customized to work with the specific folder structure of the Zarr overviews previously created. Moreover, containerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were locally deployed. The performance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of twelve colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geo-spatial data visualization context.\n\nSpeed up\nThe performance of both TiTiler services to dynamically create tiles for the different data formats was evaluated using the Speed Up metric proposed in Durbha et al. (2023) (Equation 1). In this case, the Speed Up explains how much did the process of requesting tiles sped up by using a data format A compared to using a data format B.\n\\[ SpeedUp = \\frac{t_{format A}}{t_{format B}}  \\tag{1}\\]\n\n\nZoom level influence\nFinally, the effect of the level of zoom in a web map visualization on the response times of requesting tiles from the different tiling services was evaluated by fitting an Ordinary Least Squares (OLS) univariate linear regression that followed Equation 2.\n\\[ ResponseTime = \\beta_1 \\cdot ZoomLevel + \\beta_0 + \\epsilon  \\tag{2}\\]"
  },
  {
    "objectID": "index.html#baseline-scenario",
    "href": "index.html#baseline-scenario",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\n\nCurrent workflow\nOne of the main findings of the interviews was the process followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 3 and show how complex and time consuming these tasks can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying it out.\n\n\n\n\n\n\nFigure 3: Baseline workflow\n\n\n\nAccording to Figure 3, some of the most time-consuming tasks were searching for data on Google Cloud Storage and downloading it for visualization. Additionally, seeking advice from colleagues about the dataset’s location added a major uncertainty to the time estimates, as responses varied from very quick to considerably delayed or non existent.\n\n\nThematic Content Analysis\nWhen asked about the recurrent patterns on the interviews undertaken to define the baseline scenario, the large language model used in this research found four main topics:\n\nThere is a high uncertainty on the location of datasets and a high dependency on colleagues to find them.\nMultiple sources and locations of data.\nData familiarity helps users locate data more quickly.\nUse of specific tools and methods for different datasets.\n\nAfter some refinement and a deeper analysis of the interviews, the major pitfalls found on the process of data discovery and visualization in the company were summarized as follows:\n\nHigh dependency on colleagues for dataset location.\nDisorganized structure of Google Storage Buckets.\nData familiarity helps users locate data more quickly.\nLocating data is dependent on recurrent work with a specific dataset.\nNot intuitive naming of repositories with datasets.\nUnderstanding of diverse tools to access different data is currently necessary.\nDownload of data is required in most cases to visualize it.\nNot one place where all existing data can be found.\n\nAll of these pitfalls highlight the need for a simpler data discovery implementation, where data visualization can also be integrated seamlessly. Previous studies have found that key difficulties for earth observation data discovery include heterogeneous query interfaces, and use of diverse metadata models (Miranda Espinosa et al., 2020). To address these challenges, the approach should enable easy access to datasets based on specific queries, ensuring that users can efficiently locate and utilize the data they need. By harmonizing metadata standards and query protocols through the use of STAC specifications, the process of data discovery can be greatly improved, making it more accessible and user-friendly."
  },
  {
    "objectID": "index.html#service-integration",
    "href": "index.html#service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Service integration",
    "text": "Service integration\nThe integration of the services deployed resulted in a version of STAC Browser including three different collections containing datasets related to the forest baseline created by the company, elevation data from third party organizations and a collection for the comparison of COG and Zarr data. The web application can be accessed in https://eoapi.satelligence.com/browser.\n\nEffective integration\nThe effective integration was not an easy task and involved multiple aspects, ranging from editing data formats to facilitate their visualization, transitioning from a static to a dynamic catalog, customizing APIs, and finalizing with the correct deployment of the services.\n\nData formats\nAn essential step related to data formats was the edition of Zarr datasets to achieve their optimal visualization. This edition involved creating a series of overviews of the same dataset at different resolutions. Specifically, this was accomplished by converting COGs into multiple Zarr files resampled at various spatial resolutions. These resampled Zarr files, acting as overviews enhance visualization by allowing the appropriate resolution to be accessed based on the map scale, similar to the approach used when visualizing COGs (Lynnes et al., 2020). Even though the approach followed in this study allowed for improved visualization of Zarr files, the creation of Zarr pyramids in a more optimized way is still necessary. Other researchers have been focusing their efforts on this task to enhance the efficiency and effectiveness of the process (Barciauskas et al., 2024).\n\n\nLeverage of APIs\nTo effectively query the datasets stored in the catalog, a transition from a static to a dynamic catalog (i.e. a STAC API) was needed. This shift was facilitated by the deployment of a STAC API within the eoAPI framework. The STAC API facilitated the querying capabilities of the datasets stored in the catalog by dynamically requesting datasets based on their metadata. This dynamic setup not only facilitated data discovery but also enabled the use of additional tools such as the STAC API QGIS plugin. The plugin could simplify the process of data discovery and its direct manipulation.\nFor the visualization of Zarr datasets, it was necessary to customize the TiTiler-Xarray API to accommodate the new Zarr pyramid structure. This customization involved overwriting a series of functions in the main code of the application to align with the newly created Zarr pyramids. By adapting the API to handle the specific requirements of the Zarr format and its multi-resolution overviews, the visualization process was optimized.\n\n\nDeployment\nAs described on Section 2.2.4, the deployment of both eoAPI and the additional services utilized was perform using Google Kubernetes Engine (GKE), which is K8s’ GCP service. In a GKE cluster, the setup of complex multi-service applications that connect to each other with an internal network is simplified (Gupta et al., 2021). Moreover, eoAPI simplified the deployment by providing a guide for deployment that used a Helm chart. A Helm chart is a collection of files that describe the K8s related resources needed to run a multi-service application and it can improve the speed of deployment by a factor of up to 6 times (Gokhale et al., 2021). These factors greatly influenced the decision of deploying the whole suite of services in eoAPI.\nMoreover, the performance of some of the eoAPI services deployed using K8s had been already assessed by previous studies. For instance, Munteanu et al. (2024) performed tests on a deployed version of STAC API that, like the deployment performed in this study, used PgSTAC as the backend. These authors deployed a dynamic STAC API loaded with the metadata of approximately 2.3 TB of spatial data on a K8s cluster and evaluated the performance by assessing both the response times and resources used in a hypothetical scenario where 7,000 users would perform requests simultaneously. Their results showed that these services are capable of supporting effectively a much larger amount of users than the estimated by Satelligence.\nFinally, additional to the already covered advantages, the deployment of eoAPI and the fast community driven development of new tools brings with it benefits that could become very important for S11’s workflow. For instance, the visualization of vector data using TiPg is a service included in the eoAPI deployment that wasn’t used during this internship, but should certainly be integrated in the near future by the company to visualize their supply chain datasets. Moroever, the community adopting STAC specifications has been growing fast. Due to this, a big series of STAC-extensions have been developed to fulfill the requirements of the users. During this internship, extensions to add additional metadata were included, however, due to time constraints other extensions that could prove beneficial for the company were not integrated. Specifically, the possibility of adding an authentication layer to the catalog still needs to be done and should be the next step to ensure the privacy of the data.\n\n\n\nWorkflow improvement\nOnce the deployment of eoAPI and the extra services was done, a new workflow for both the new data discovery and visualization tasks was designed and is presented on Figure 4. This new workflow shows a clear improvement on the speed and the ease of use of the new methods employed.\n\n\n\n\n\n\nFigure 4: New data discovery and visualization workflow\n\n\n\nMoreover, it can be seen that with the new implementation most of the issues identified on the TCA were addressed. There is no longer a dependency on colleagues for locating datasets, as all data is now consolidated in one place. The disorganized structure of Google Storage Buckets is no longer a concern since the catalog can integrate data stored in multiple buckets into a single, cohesive STAC collection. The previous issue of non-intuitive naming conventions for data repositories, is resolved because it is unnecessary to know the data source once it is included in the STAC catalog. Furthermore, there is no longer a need to understand diverse tools for accessing different data; the STAC Browser facilitates querying collections and visualizing items. Finally, the STAC catalog serves as the centralized location for all data used in S11 workflows, which favors long term usability of code that relies on this data."
  },
  {
    "objectID": "index.html#performance-of-multi-format-data-visualization",
    "href": "index.html#performance-of-multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nThe results of the experiments made with different cloud-optimized data formats are presented in two subsections. The first subsection evaluates the overall performance of the two data formats and the second subsection assesses the performance of these data formats based on different zoom levels.\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-Xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented interactively on Figure 5 and summarized in Figure 6. In the figure it can be observed that on average the response for requests of COG tiles was 2.53 times faster than the one for the same file in ZARR format. Moreover, Figure 6 shows that the response times for tiles created from data stored as Zarr showed a considerable wider range than the ones generated from data in the COG format, which indicates more variability in the performance for Zarr.\n\nInteractive comparison\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 5: Interactive comparison of rendering speed for COG and Zarr data formats\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Response times for tile requests depending on data format\n\n\n\n\n\nThe results obtained are coherent to the ones previously shown by IMPACT (2023), where COGs’ rendering time was found to be lower than the one for Zarr files at different zoom levels. These results show that COGs, being specifically optimized for spatial data visualization, offer faster visualization compared to Zarr files. However, this does not take away from the fact that Zarr provides additional benefits, such as the ability to store n-dimensional arrays. Furthermore, recent advancements like GeoZarr and the creation of Zarr pyramids with new packages like ndpyramid could bring significant improvements to this data format Barciauskas et al. (2024).\n\n\nFine tuning of dataset\nEven though COGs showed to be very performant, their performance could be further enhanced by tuning specific GDAL parameters. These adjustments could improve the speed of tiling services using COG (IMPACT, 2023). Future considerations should include optimizing these parameters to maximize efficiency (See performance tuning section).\n\n\n\nEffects of zoom level\nAs seen on Figure 7, the zoom level of the map showed an effect on the time spent requesting and getting a tile from a tiling service for the COG format. In this study, it was found that the request times decreased by 0.01 seconds per zoom level for COGs, and didn’t show a notable change for Zarrs (+ 0.002 seconds per zoom level). The behavior presented here for COGs differs from the one observed by IMPACT (2023), where no difference in rendering time was observed as a function of the zoom level. This difference could be explained by the fact that in their study, only the lowest zoom levels were considered, while in this study only the highest zoom levels were taken into account, however, to verify this hypothesis a broader study of the response times at more zoom levels should be performed. This was not done in this study because the limited size of the study area that the raster images covered only allowed visualization of the data at high zoom levels (i.e., above 8).\n\n\n\n\n\n\n\n\nFigure 7: Request times depending on zoom level and their respective trends for COG and Zarr data formats.\n\n\n\n\n\nMoreover, as seen on Figure 8, while the size of blocks remain constant throughout all of the overviews in the COG file, the sizes of the Zarr chunks varied in the pyramids created. Due to this, the tiles requested at higher zoom levels were larger than the ones requested at lower zoom levels which could explain the difference between the trends observed in Figure 7 for the two data formats.\nFinally, despite efforts to mitigate caching effects, tiles generated from the same overview might have been reused across different zoom levels, potentially contributing to the reduced times observed for COGs at higher zoom levels. The COG used in this study had a maximum zoom level of 14. Beyond this level, tiles are generated using the same overview (i.e. the one with the highest resolution), which could indicate caching. Although both tiling services used in the study implement caching, a more in-depth investigation is needed to fully understand its impact on performance for both data formats.\n\n\n\n\n\n\n\n\nFigure 8: Variation of chunk sizes in Zarr file depending on zoom level compared to a constant block size for COG data format."
  },
  {
    "objectID": "index.html#sec-baseline-q",
    "href": "index.html#sec-baseline-q",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario questionnaire",
    "text": "Baseline scenario questionnaire\n\nRelated to data discovery\n\nI am working for Wilmar in South East Asia. Do you know what is the forest baseline that I should use and where can I find it?\nI have been checking the results of the Soy map we created. Do you know which DEM was used for it? And where can I find it?\nDo you know which DEM is used as the terrain mask when using Sentinel 1 data?\nI need to access the concessions data provided by Grepalma. Where can I find it?\n\n\n\nRelated to data visualization\n\nI am interested on getting an overview of where was the primary forest present in Colombia in 2007. Could you visualize a layer with this data for me?\nI need to visualize the concessions provided by fedepalma. Could you do it for me?"
  },
  {
    "objectID": "index.html#sec-gpt-prompt",
    "href": "index.html#sec-gpt-prompt",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Thematic Content Analysis prompt",
    "text": "Thematic Content Analysis prompt\nI will give you some notes I took from an interview I did to four study subjects:\nW, X, Y and Z.\n\nTell me if you identify any themes or topics that are repeated in the notes that \nI took from the answers of the individuals. In other words, do a simple Thematic\nContent Analysis of the interviews."
  },
  {
    "objectID": "index.html#sec-request-code",
    "href": "index.html#sec-request-code",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\nDisclaimer: In order to run the code presented below, the user must have authenticated their Google account and have the TiTiler-PgSTAC and the TiTiler-Xarray services running on localhost:8082 and localhost:8084 respectively.\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 7:\n        change_range = 1\n    elif z &lt;= 9:\n        change_range = 5\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11\n# Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\",\n        \"_name=gray&rescale=0,70\",\"_name=gray_r&rescale=0,70\",\n        \"_name=jet&rescale=0,90\", \"_name=jet_r&rescale=0,90\",\n        \"_name=hot&rescale=0,80\", \"_name=hot_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n        url_zarr = \"https://localhost:8084/tiles/WebMercatorQuad/\"+\\\n        f\"{tile}%401x?url=gs://s11-tiles/zarr/separate&\"+\\\n        \"variable=band_data&reference=false&decode_times=true&\"+\\\n        f\"consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"https://localhost:8082/collections/\"+\\\n        f\"Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/\"+\\\n        f\"WebMercatorQuad/{tile}%401x?bidx=1&assets=data&\"+\\\n        \"unscale=false&resampling=nearest&reproject=nearest&\"+\\\n        f\"colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter_k8.csv')"
  }
]