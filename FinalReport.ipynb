{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Cataloguing and visualizing big Geodata\n",
        "subtitle: Final report\n",
        "author:\n",
        "  - name: Martín Domínguez Durán\n",
        "    affiliations:\n",
        "      - name: Wageningen University & Research\n",
        "        address: Wageningen, The Netherlands\n",
        "bibliography: references.bib\n",
        "csl: apa.csl\n",
        "format: \n",
        "  html: \n",
        "    fig-width: 8\n",
        "    fig-height: 6\n",
        "  titlepage-pdf:\n",
        "    fig-pos: 'H'\n",
        "    fig-width: 200\n",
        "    link-citations: true\n",
        "    documentclass: scrbook\n",
        "    classoption: [\"oneside\", \"open=any\"]\n",
        "    number-sections: true\n",
        "    toc: true\n",
        "    lof: true\n",
        "    # lot: true\n",
        "    titlepage: \"bg-image\"\n",
        "    titlepage-bg-image: \"img/corner-bg.png\"\n",
        "    titlepage-logo: \"img/logo.png\"\n",
        "    titlepage-header: \"The Publisher\"\n",
        "    titlepage-footer: |\n",
        "      **Registration number:** 1254246\\\n",
        "      **Period of Internship:** 2024-04-08 - 2024-08-08\\\n",
        "      **Date final report:** 2024-07-31\\\n",
        "      **Telephone number student:** +31651120353\\\n",
        "      **Name of Company:** Satelligence B.V.\\\n",
        "      **Host supervisor:** Luca Foresta\\\n",
        "      **MGI supervisor:** Lukasz Grus\n",
        "    keep-tex: true\n",
        "---"
      ],
      "id": "f65300ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "\n",
        "## Internship organization background\n",
        "\n",
        "Satelligence is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free regulations, such as the EUDR [@satelligence_internship_2023].\n",
        "\n",
        "## Context and justification of research\n",
        "\n",
        "Satelligence currently employs cloud computing, specifically Kubernetes, to process extensive volumes of satellite imagery amounting to terabytes. While this processing workflow currently runs smoothly, the company’s data team faces challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data [@satelligence_internship_2023]. Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights [@giri_big_2014]. In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n",
        "\n",
        "### Significance of the topic\n",
        "\n",
        "In the past decades there has been a rapid increase in the amount and size of geospatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day [@zhao_scalable_2021]. This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\n",
        "\n",
        "Traditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data [@rajabifard_spatial_2001]. However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility [@durbha_advances_2023]. Due to this, an important shift is underway towards more cloud-based SDIs. These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n",
        "\n",
        "#### Cloud-based data storage\n",
        "\n",
        "Spatial data, just like any other type of data, can be catalogued into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) [@mishra_structured_2017]. The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. With the help of Structured Query Language (SQL)) [@kaufmann_database_2023]. In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\n",
        "\n",
        "The emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes [@durbha_advances_2023].\n",
        "\n",
        "Cloud-Optimized GeoTIFFs ([COGs](https://www.cogeo.org/)) are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored and storing different image tiles for different scales. These characteristics make COGs heavier than traditional image formats, however, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles [@durbha_advances_2023].\n",
        "\n",
        "Another cloud native data format that has gained popularity recently is [Zarr](https://zarr.readthedocs.io/en/stable/). This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr differently than COGs store the metadata separately from the data chunks. Normally, using external JSON files [@durbha_advances_2023].\n",
        "\n",
        "#### Data discovery services\n",
        "\n",
        "A discovery service widely used for the exploration of big geodata (e.g. COGs) is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatial data, STAC simplifies the management and discovery of big geodata [@brodeur_geographic_2019]. This service works by organizing the data into catalogs, collections, items, and assets that will only be read when a computation is required [@durbha_advances_2023]. This feature optimizes workflows by reducing unnecessary data loading.\n",
        "\n",
        "##### STAC fundamentals\n",
        "\n",
        "A Catalog built under STAC specifications is composed by:\n",
        "\n",
        "| **STAC components** | **Description**                                                                                          |\n",
        "|----------------------------------------|--------------------------------|\n",
        "| *Assets*            | An asset can be any type of data with a spatial and a temporal component.                                |\n",
        "| *Items*             | An item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket) |\n",
        "| *Collections*       | Defines a set of common fields to describe a group of Items that share properties and metadata           |\n",
        "| *Catalogs*          | Contains a list of STAC collections, items or can also contain child catalogs.                           |\n",
        "\n",
        ": STAC components {.striped .hover}\n",
        "\n",
        "#### Visualization interfaces\n",
        "\n",
        "The visualization of spatial data brings with it a series of challenges due to its big nature. Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by dynamically generating PNG/JPEG image tiles only when requested without reading the entire source file into memory [@noauthor_titiler_nodate]. This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\n",
        "\n",
        "### Added value of this research\n",
        "\n",
        "This research aims to develop a STAC catalog to facilitate the discovery of diverse big geospatial datasets created by different teams within the company. Depending on the time available for the thesis, data visualization functionalities may be integrated. Moreover, the research will explore tools to efficiently manage a STAC catalog containing both static and dynamic datasets.\n",
        "\n",
        "## Research questions\n",
        "\n",
        "- What are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\n",
        "- How does the integration of cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications influence the process and experiences of discovering big spatial data?\n",
        "- To what extent do dynamic tiling services can perform in visualizing different cloud-optimized data formats?\n",
        "<!-- To what extent does the integration of cloud optimized data formats and Spatio-Temporal Asset Catalog (STAC) specifications can reduce times needed for discovering big spatial data produced by the company? -->\n",
        "<!-- -   How can a STAC catalog be structured and updated to improve the discoverability of datasets within a mix of static and dynamic datasets? -->\n",
        "\n",
        "\n",
        "\n",
        "# Methodology\n",
        "\n",
        "To address the proposed research questions, a series of activities and the deliverables expected from them are presented on @fig-fc. Additionally, a detailed description of each activity is presented in the following subsections.\n",
        "\n",
        "![Internship flowchart](img/FlowChart_Internship.png){#fig-fc width=\"60%\"}\n",
        "\n",
        "## Data and processing code familiarization\n",
        "\n",
        "In this step, I will familiarize myself with the current tools used for the processing of the images and its storage. This step will include the understanding of cloud services and internal image processing tools and the main datasets to be referenced on the catalog (STAC data). Moreover, in this step an initial description of the STAC data metadata will be performed.\n",
        "\n",
        "### Cloud services familiarization\n",
        "\n",
        "An overview of the cloud services used by the company will be described. This will be mainly with the objective to understand, but not limited to:\n",
        "\n",
        "-    Who access the data?\n",
        "\n",
        "-   What are the costs of accessing it?\n",
        "\n",
        "-   How often certain data is updated?\n",
        "\n",
        "-   How is the data updated?\n",
        "\n",
        "## Baseline scenario definition\n",
        "\n",
        "The baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. The data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\n",
        "\n",
        "The questions asked during the interviews were oriented towards two main topics that were covered during this internship: Spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in appendix **###** and were meant to be open questions with multiple possible answers. \n",
        "\n",
        "## Data integration\n",
        "\n",
        "### Local STAC creation and browsing\n",
        "\n",
        "This step will mainly be focused on the set up of the developing environment to both create a local STAC catalog and browse through it. This will include:\n",
        "\n",
        "1. Creation of a virtual environment or Docker container with all the required packages to create a STAC catalog.\n",
        "2. Creation of Local STAC using sample data from the company.\n",
        "3. Browse through the STAC catalog using tools like STAC browser.\n",
        "\n",
        "The **deliverable** of this step will be a GitLab repository with code to create a catalog, add assets from a local directory and browse through them locally using STAC browser.\n",
        "\n",
        "### Organization of main STAC structure & extensions per asset {#sec-structure}\n",
        "\n",
        "In this step the structure of the STAC catalog will be defined. This will involve the selection of datasets that will be referenced on the catalog, the definition of subcatalogs and/or collections to group items with similar metadata. An initial idea of the structure of the main STAC catalog can be seen on @fig-stac-str. Initially, the creation of two different subcatalogs is proposed to keep the static and dynamic dataset separated. Moreover, the selection of the [STAC extensions](https://stac-extensions.github.io/)[^1] used for each dataset will be defined in this step. \n",
        "\n",
        "![Initial proposed STAC structure](img/STAC_Satelligence_structure.png){#fig-stac-str width='90%'}\n",
        "\n",
        "[^1]: STAC extensions are additional metadata properties that can be added to a dataset. (e.g. Classes, bands, sensor-type, etc.)\n",
        "\n",
        "### Build main STAC v.0.1\n",
        "\n",
        "This step will focus on the building of the initial version of the main STAC catalog, once the datasets and the overall structure has been defined. It will involve the population of the catalog with STAC components following the defined structure on @sec-structure. Furthermote, on this step a series of validation tools will be used to check that the STAC catalog created is followins the STAC spcification. These tools are part of the python package [stac-tools](https://github.com/stac-utils/stactools). \n",
        "\n",
        "The **deliverable** of this step will be a GitLab repository with code to create a catalog, create collections, add assets from a directory on the cloud and update them.\n",
        "\n",
        "### Set up STAC browser on GCP\n",
        "\n",
        "In this step a version of the STAC Browser application will be deployed using the tools from Google Cloud Platform (GCP). This application will allow users to browse and interact with the STAC catalog through a user-friendly interface. Additionally, this step will involve the definition of resources and tools from GCP that will be employed to deploy the application. For instance, the decision of doing it through a virtual machine or on a containerized way will be made.\n",
        "\n",
        "The **deliverable** of this step will be a the STAC browser application running on GCP.\n",
        "\n",
        "### Automate processes via CI pipeline\n",
        "\n",
        "Finally, the code to create, modify and/or deploy the STAC catalog will be merged into a continuous integration pipeline that will allow the integration of this catalog with other tools from the company. For instance, the Distributed Processing Framework (DPROF), which is satelligence's Satellite Data Processing engine. \n",
        "\n",
        "<!-- ### Visualization tool development (Optional)\n",
        "\n",
        "Finally, if time allows, an internal application will be developed to access and visualize the data from the STAC catalog created.\n",
        "\n",
        "Required libraries:\n",
        "\n",
        "-   [Streamlit](https://docs.streamlit.io/) for user interface\n",
        "\n",
        "-   [Leafmap](https://leafmap.org/) for spatial data visualization\n",
        "\n",
        "**Deliverable:** Containerized application to visualize the data  -->\n",
        "\n",
        "## Performance assessment\n",
        "\n",
        "The assessment of the performance of the new Data Catalog will be measured using the baseline scenario established at the beginning of the internship and the Speed Up metric proposed by [@durbha_advances_2023]:\n",
        "\n",
        "$$ SpeedUp = \\frac{t_{baseline}}{t_{catalog}} $$\n",
        "\n",
        "This metric explains how much the process to access data has sped up thanks to the integration of cloud-based storage, the data catalog and the browsing interface.\n",
        "\n",
        "\n",
        "\n",
        "# Results\n",
        "\n",
        "## Baseline scenario\n",
        "\n",
        "### Main findings\n",
        "\n",
        "The main finding of the interviews were the steps followed currently to discover, retrieve and visualize data. These steps are summarized on @fig-baseline and show how complex and time consuming the process of discovering and visualizing spatial data can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying out.\n",
        "\n",
        "![Baseline workflow](img/Baseline_data_discovery_workflow.png){#fig-baseline width=\"100%\"}\n",
        "\n",
        "The major pitfalls found on the process of data discovery in the company could be summarized in ....\n",
        "\n",
        "## Service integration\n",
        "\n",
        "*Explain here how eoAPI uses multiple services, how each of them helps S11 in their data discovery and vizz tasks, and how did I manage to deploy it*\n",
        "\n",
        "Kubernetes\n",
        "\n",
        "STAC-API, pgSTAC, TiTiler\n",
        "\n",
        "## Multi-format data visualization\n",
        "\n",
        "TiTiler-PgSTAC & TiTiler-xarray\n",
        "\n",
        "## Performance assessment\n",
        "\n",
        "### Data discovery\n",
        "\n",
        "### Data visualization\n",
        "IDEA: GET requests and time them\n"
      ],
      "id": "ff266457"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: \"Request times depending on zoom level\"\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "COG_t = [2.97, 3.93, 3.45]\n",
        "ZARR_t = [5.90, 7.74, 6.43]\n",
        "\n",
        "data = pd.DataFrame([COG_t, ZARR_t]).T\n",
        "data.columns = ['COG', 'ZARR']\n",
        "\n",
        "fig = plt.figure(figsize= (10,5))\n",
        "\n",
        "ax = sns.boxplot(data, palette = 'deep')\n",
        "sns.despine(trim = True, offset = -10)\n",
        "\n",
        "a = ax.set_ylabel('Render time [s]')\n",
        "\n",
        "print(\"Speed up (COG)\", data['ZARR'].mean()/data['COG'].mean())"
      ],
      "id": "aecfc6f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- \n",
        "import requests\n",
        "import random\n",
        "\n",
        "tiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \"17/102503/65134\", \"18/205062/130211\"]\n",
        "\n",
        "def modify_tile(tile):\n",
        "    parts = tile.split('/')\n",
        "    z = int(parts[0])\n",
        "    x = int(parts[1])\n",
        "    y = int(parts[2])\n",
        "\n",
        "    # Determine the range of change based on the value of z\n",
        "    if z <= 9:\n",
        "        change_range = 2\n",
        "    elif z <= 12:\n",
        "        change_range = 5\n",
        "    elif z <= 15:\n",
        "        change_range = 10\n",
        "    elif z <= 18:\n",
        "        change_range = 50\n",
        "\n",
        "    # Apply the change to x and y\n",
        "    x_change = random.randint(-change_range, change_range)\n",
        "    y_change = random.randint(-change_range, change_range)\n",
        "\n",
        "    new_x = x + x_change\n",
        "    new_y = y + y_change\n",
        "\n",
        "    # Return the modified tile as a string\n",
        "    return f\"{z}/{new_x}/{new_y}\"\n",
        "\n",
        "times_zarr = []\n",
        "times_cog = []\n",
        "\n",
        "z_level = []\n",
        "\n",
        "cmap = [\"=[[[1.0,1.1],[0,47,0,255]],[[2.0,2.1],[55,76,33,255]],[[3.0,3.1],[105,140,60,255]],[[4.0,4.1],[178,199,140,255]],[[5.0,5.1],[164,198,121,255]],[[6.0,6.1],[198,112,85,255]],[[7.0,7.1],[170,219,167,255]],[[8.0,8.1],[87,162,164,255]],[[50.0,50.1],[255,183,1,255]],[[52.0,52.1],[238,223,201,255]],[[53.0,53.1],[185,120,119,255]],[[54.0,54.1],[218,170,241,255]],[[55.0,55.1],[40,205,167,255]],[[60.0,60.1],[208,227,243,255]],[[66.0,66.1],[166,219,204,255]],[[70.0,70.1],[255,255,255,255]],[[71.0,71.1],[185,136,94,255]],[[72.0,72.1],[125,165,142,255]],[[74.0,74.1],[188,85,123,255]],[[90.0,90.1],[241,195,132,255]]]\",\"_name=greens_r&rescale=0,70\"]\n",
        "\n",
        "for i in range(1):\n",
        "\n",
        "    mod_tiles = [modify_tile(tile) for tile in tiles]\n",
        "    k = int(i/2-i//2 + 0.5)\n",
        "\n",
        "    for tile in mod_tiles:\n",
        "\n",
        "        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/{tile}%401x?url=gs://s11-tiles/zarr/separate&variable=band_data&reference=false&decode_times=true&consolidated=true&colormap{cmap[k]}&return_mask=true\"\n",
        "\n",
        "        url_cog = f\"http://localhost:8082/collections/Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/WebMercatorQuad/{tile}%401x?bidx=1&assets=data&unscale=false&resampling=nearest&reproject=nearest&colormap{cmap[k]}&return_mask=true\"\n",
        "\n",
        "        x = requests.get(url_zarr)\n",
        "        times_zarr.append(x.elapsed.total_seconds())\n",
        "\n",
        "        x = requests.get(url_cog)\n",
        "        times_cog.append(x.elapsed.total_seconds())\n",
        "\n",
        "        z_level.append(int(tile.split('/')[0]))\n",
        "\n",
        "sns.set_style('ticks')\n",
        "\n",
        "fig = plt.figure(figsize = (10,7))\n",
        "\n",
        "sns.regplot(x = z_level, y=times_cog)\n",
        "sns.regplot(x = z_level, y=times_zarr)\n",
        "\n",
        "Z_level = [12, 18, 14]\n",
        " -->\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Discussion\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Future work\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Internship planning\n",
        "\n",
        "**Internship duration:** 08/04/2024 - 08/08/2024 (4 months)\n",
        "\n",
        "# References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "2786ee98"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}