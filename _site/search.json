[
  {
    "objectID": "FinalReport.html",
    "href": "FinalReport.html",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "",
    "text": "The abstract goes here."
  },
  {
    "objectID": "FinalReport.html#internship-organization-background",
    "href": "FinalReport.html#internship-organization-background",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Internship organization background",
    "text": "Internship organization background\nSatelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning (Satelligence, n.d.). More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) (Satelligence, 2023). S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "FinalReport.html#context-and-justification-of-research",
    "href": "FinalReport.html#context-and-justification-of-research",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data (Satelligence, 2023). Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights (Giri and Lone, 2014). In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day (Zhao et al., 2021). This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data (Rajabifard and Williamson, 2001). However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility (Durbha et al., 2023). Due to this, an important shift is underway towards more cloud-based SDIs (Tripathi et al., 2020). These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) (Mishra and Misra, 2017). The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. With the help of Structured Query Language (SQL)) (Kaufmann and Meier, 2023). In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes (Durbha et al., 2023).\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\nCloud-optimized data formats\nCOG\nCloud-Optimized GeoTIFFs (COGs) are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests (Desruisseaux et al., 2021). Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitate data interoperability (OGC, 2023).\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files (Durbha et al., 2023). Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service (Desruisseaux et al., 2021). Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC (Chester, 2024).\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data (Brodeur et al., 2019). This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) (Durbha et al., 2023).\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as APIs that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data (RadiantEarth, 2024).\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other. In the case of the STAC API, it provides endpoints for searching and retrieving geo-spatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geo-spatial community. Moreover, even though STAC API is not an OGC standard or a OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning) (Holmes, 2021).\n\n\nVisualization interfaces\nThe visualization of spatial data brings with it a series of challenges due to its big nature. Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory (TiTiler, n.d.). This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), Cloud Optimized GeoTIFFs (COGs), and is currently working on adding support for Zarrs. For the first two the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the handling of multidimensional data arrays.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geo-spatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "FinalReport.html#research-questions",
    "href": "FinalReport.html#research-questions",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow can cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications be integrated to enhance the process and experiences of discovering big spatial data within the company?\nTo what extent do dynamic tiling services can perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "FinalReport.html#data-and-processing-code-familiarization",
    "href": "FinalReport.html#data-and-processing-code-familiarization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data and processing code familiarization",
    "text": "Data and processing code familiarization\nIn this step, I will familiarize myself with the current tools used for the processing of the images and its storage. This step will include the understanding of cloud services and internal image processing tools and the main datasets to be referenced on the catalog (STAC data). Moreover, in this step an initial description of the STAC data metadata will be performed.\n\nCloud services familiarization\nAn overview of the cloud services used by the company will be described. This will be mainly with the objective to understand, but not limited to:\n\nWho access the data?\nWhat are the costs of accessing it?\nHow often certain data is updated?\nHow is the data updated?"
  },
  {
    "objectID": "FinalReport.html#baseline-scenario-definition",
    "href": "FinalReport.html#baseline-scenario-definition",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario definition",
    "text": "Baseline scenario definition\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. The data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: Spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in appendix Section 6.1 and were meant to be open questions with multiple possible answers."
  },
  {
    "objectID": "FinalReport.html#data-integration",
    "href": "FinalReport.html#data-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data integration",
    "text": "Data integration\nBriefly describe what will be included in the section: Explanation of the datasets included in the catalog, the building of the catalog (s11-cats)\n\nDatasets included\n\n\nS11-cats repository\nDescription of modules\n\n\nPgSTAC\n\n\neoAPI\n\nUse of docker containers to run individual applications that can connect to each other.\n\n\n\nCI pipeline"
  },
  {
    "objectID": "FinalReport.html#performance-assessment",
    "href": "FinalReport.html#performance-assessment",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance assessment",
    "text": "Performance assessment\nThe assessment of the performance of the new Data Catalog will be measured using the baseline scenario established at the beginning of the internship and the Speed Up metric proposed by (Durbha et al., 2023):\n\\[ SpeedUp = \\frac{t_{baseline}}{t_{catalog}} \\]\nThis metric explains how much the process to access data has sped up thanks to the integration of cloud-based storage, the data catalog and the browsing interface."
  },
  {
    "objectID": "FinalReport.html#baseline-scenario",
    "href": "FinalReport.html#baseline-scenario",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. The data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: Spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in Section 6.1 and were meant to be open questions with multiple possible answers."
  },
  {
    "objectID": "FinalReport.html#service-integration",
    "href": "FinalReport.html#service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Service integration",
    "text": "Service integration\nExplain here how eoAPI uses multiple services, how each of them helps S11 in their data discovery and vizz tasks, and how did I manage to deploy it\nKubernetes\nSTAC-API, pgSTAC, TiTiler\n\nData discovery improvement\nFlowchart with STAC"
  },
  {
    "objectID": "FinalReport.html#multi-format-data-visualization",
    "href": "FinalReport.html#multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing Cloud Optimized GeoTIFFs (COGs) and Zarr data formats, the following approach was undertaken. Firstly, a COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This preprocessing step, completed by the company prior to the study, ensured that the same data was used across both data formats, allowing for direct comparison. Then, the TiTiler-Xarray service was then customized to work with the specific folder structure of the ZARR overviews previously created. Moreover, containerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were deployed locally. The performance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of six colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geospatial data visualization context.\n\nSpeed up\nThe performance of both TiTiler services to dynamically create tiles for the different data formats was evaluated using the Speed Up metric proposed in Durbha et al. (2023) (Equation 1). In this case, the Speed Up explains how much did the process of requesting tiles sped up by using a data format A compared to using a data format B.\n\\[ SpeedUp = \\frac{t_{format A}}{t_{format B}}  \\tag{1}\\]\n\n\nZoom level influence\nFinally, the effect of the level of zoom in a web map visualization on the response times of requesting tiles from the different tiling services was evaluated by fitting an Ordinary Least Squares (OLS) univariate linear regression that followed Equation 2.\n\\[ ResponseTime = \\beta_1 \\cdot ZoomLevel + \\beta_0 + \\epsilon  \\tag{2}\\]"
  },
  {
    "objectID": "FinalReport.html#performance-assessment-1",
    "href": "FinalReport.html#performance-assessment-1",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance assessment",
    "text": "Performance assessment\n\nData discovery\n\n\nData visualization\nIDEA: GET requests and time them\n\n\nSpeed up (COG) 1.9391304347826084\n\n\n\n\n\nRequest times depending on zoom level"
  },
  {
    "objectID": "FinalReport.html#gantt-chart",
    "href": "FinalReport.html#gantt-chart",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Gantt chart",
    "text": "Gantt chart\n\n\n\n\n\ngantt\n    title Internship Gantt Chart\n    dateFormat  DD-MM-YYYY\n    excludes weekends\n    todayMarker off\n    axisFormat %d-%m\n\n    section Proposal\n    Writing & reviewing           :active, 08-04-2024, 29-04-2024\n\n    section Data & Code familiarization\n    General code      :29-04-2024, 4d\n    Cloud servives    :01-05-2024, 4d\n\n    section Baseline scenario determination\n    Define the components of a baseline scenario  : 06-05-2024, 1d\n    Test the baseline times                       : 07-05-2024, 3d\n\n    section Data integration\n    Local STAC creation & Browsing              : done, 08-04-2024, 25-04-2024\n    Main STAC structure                         : main_s, 10-05-2024, 2d\n    Build main STAC v0.1                        : after main_s, 21d\n    Set up STAC browser                         : stac_b, 28-05-2024, 15d\n    Automation via CI pipeline                  : end_data_int, after stac_b, 15d\n    Refinement                                  : after end_data_int, 21d\n\n    section Performance assessment\n    Evaluate new discovery times                : after stac_b, 2d\n\n    section Report writing\n    Writing                                     : 29-04-2024, 19-07-2024\n    Review                                      : 19-07-2024, 26-07-2024\n    Reflection paper writing                    : 26-07-2024, 07-08-2024\n\n    section Important dates\n    Midterm evaluation                          :milestone, 10-06-2024, 24h\n    Submission of report                        :milestone, 26-07-2024, 24h\n    Final examination                           :milestone, 22-08-2024, 24h"
  },
  {
    "objectID": "FinalReport.html#footnotes",
    "href": "FinalReport.html#footnotes",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSTAC extensions are additional metadata properties that can be added to a dataset. (e.g. Classes, bands, sensor-type, etc.)↩︎"
  },
  {
    "objectID": "FinalReport.html#performance-of-multi-format-data-visualization",
    "href": "FinalReport.html#performance-of-multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nTiTiler-PgSTAC & TiTiler-xarray\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented on Figure 3. In the figure it can be obesrved that COG tiles are requested 2.38 times faster than the same file in ZARR format.\n\n\n\n\n\n\n\n\nFigure 3: Request times depending on data format and zoom level\n\n\n\n\n\n\n\nEffects of zoom level\nAs seen on Figure 4, the zoom level of the map will have an effect on the time spent requesting and getting a tile from a tile server. In this study, it was found that the request times decreased by a factor of -0.013 and -0.005 per zoom level for COGs and ZARRs respectively.\n\n\n\n\n\n\n\n\nFigure 4: Request times depending on zoom level"
  },
  {
    "objectID": "FinalReport.html#code-to-evaluate-request-times",
    "href": "FinalReport.html#code-to-evaluate-request-times",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 9:\n        change_range = 3\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11\n# Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n\n        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/\"+\\\n        \"{tile}%401x?url=gs://s11-tiles/zarr/separate&\"+\\\n        \"variable=band_data&reference=false&decode_times=true&\"+\\\n        \"consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"http://localhost:8082/collections/\"+\\\n        \"Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/\"+\\\n        \"WebMercatorQuad/{tile}%401x?bidx=1&assets=data&\"+\\\n        \"unscale=false&resampling=nearest&reproject=nearest&\"+\\\n        \"colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter.csv')"
  },
  {
    "objectID": "index.html#internship-organization-background",
    "href": "index.html#internship-organization-background",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Internship organization background",
    "text": "Internship organization background\nSatelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) [@satelligence_internship_2023]. S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "index.html#context-and-justification-of-research",
    "href": "index.html#context-and-justification-of-research",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data [@satelligence_internship_2023]. Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights [@giri_big_2014]. In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day [@zhao_scalable_2021]. This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data [@rajabifard_spatial_2001]. However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility [@durbha_advances_2023]. Due to this, an important shift is underway towards more cloud-based SDIs [@tripathi_cloud_2020]. These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) [@mishra_structured_2017]. The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. With the help of Structured Query Language (SQL)) [@kaufmann_database_2023]. In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes [@durbha_advances_2023].\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\n\nCloud-optimized data formats\n\nCOG\nCloud-Optimized GeoTIFFs (COGs) are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests [@desruisseaux_ogc_2021]. Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitat data interoperability [@ogc_ogc_2023].\n\n\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files [@durbha_advances_2023]. Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service [@desruisseaux_ogc_2021]. Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC [@chester_ogc_2024].\n\n\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data [@brodeur_geographic_2019]. This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) [@durbha_advances_2023].\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as APIs that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data [@noauthor_stac-specbest-practicesmd_nodate].\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other. In the case of the STAC API, it provides endpoints for searching and retrieving geospatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geospatial community. Moreover, even though STAC API is not an OGC standard or a OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning).\n\n\nVisualization interfaces\n\nThe visualization of spatial data brings with it a series of challenges due to its big nature. Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory [@noauthor_titiler_nodate]. This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), Cloud Optimized GeoTIFFs (COGs), and is currently working on adding support for Zarrs. For the first two the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the handling of multidimensional data arrays.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geospatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow does the integration of cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications influence the process and experiences of discovering big spatial data?\nTo what extent do dynamic tiling services can perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "index.html#data-and-processing-code-familiarization",
    "href": "index.html#data-and-processing-code-familiarization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data and processing code familiarization",
    "text": "Data and processing code familiarization\nIn this step, I will familiarize myself with the current tools used for the processing of the images and its storage. This step will include the understanding of cloud services and internal image processing tools and the main datasets to be referenced on the catalog (STAC data). Moreover, in this step an initial description of the STAC data metadata will be performed.\n\nCloud services familiarization\nAn overview of the cloud services used by the company will be described. This will be mainly with the objective to understand, but not limited to:\n\nWho access the data?\nWhat are the costs of accessing it?\nHow often certain data is updated?\nHow is the data updated?"
  },
  {
    "objectID": "index.html#baseline-scenario-definition",
    "href": "index.html#baseline-scenario-definition",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario definition",
    "text": "Baseline scenario definition\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. The data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: Spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in appendix ### and were meant to be open questions with multiple possible answers."
  },
  {
    "objectID": "index.html#data-integration",
    "href": "index.html#data-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data integration",
    "text": "Data integration\n\nLocal STAC creation and browsing\nThis step will mainly be focused on the set up of the developing environment to both create a local STAC catalog and browse through it. This will include:\n\nCreation of a virtual environment or Docker container with all the required packages to create a STAC catalog.\nCreation of Local STAC using sample data from the company.\nBrowse through the STAC catalog using tools like STAC browser.\n\nThe deliverable of this step will be a GitLab repository with code to create a catalog, add assets from a local directory and browse through them locally using STAC browser.\n\n\nOrganization of main STAC structure & extensions per asset\nIn this step the structure of the STAC catalog will be defined. This will involve the selection of datasets that will be referenced on the catalog, the definition of subcatalogs and/or collections to group items with similar metadata. An initial idea of the structure of the main STAC catalog can be seen on Figure 2. Initially, the creation of two different subcatalogs is proposed to keep the static and dynamic dataset separated. Moreover, the selection of the STAC extensions1 used for each dataset will be defined in this step.\n\n\n\n\n\n\nFigure 2: Initial proposed STAC structure\n\n\n\n\n\nBuild main STAC v.0.1\nThis step will focus on the building of the initial version of the main STAC catalog, once the datasets and the overall structure has been defined. It will involve the population of the catalog with STAC components following the defined structure on Section 2.3.2. Furthermote, on this step a series of validation tools will be used to check that the STAC catalog created is followins the STAC spcification. These tools are part of the python package stac-tools.\nThe deliverable of this step will be a GitLab repository with code to create a catalog, create collections, add assets from a directory on the cloud and update them.\n\n\nSet up STAC browser on GCP\nIn this step a version of the STAC Browser application will be deployed using the tools from Google Cloud Platform (GCP). This application will allow users to browse and interact with the STAC catalog through a user-friendly interface. Additionally, this step will involve the definition of resources and tools from GCP that will be employed to deploy the application. For instance, the decision of doing it through a virtual machine or on a containerized way will be made.\nThe deliverable of this step will be a the STAC browser application running on GCP.\n\n\nAutomate processes via CI pipeline\nFinally, the code to create, modify and/or deploy the STAC catalog will be merged into a continuous integration pipeline that will allow the integration of this catalog with other tools from the company. For instance, the Distributed Processing Framework (DPROF), which is satelligence’s Satellite Data Processing engine."
  },
  {
    "objectID": "index.html#multi-format-data-visualization",
    "href": "index.html#multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing Cloud Optimized GeoTIFFs (COGs) and Zarr data formats, the following approach was undertaken. A COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This preprocessing step, completed by the company prior to the study, ensured that the same data was used across both data formats, allowing for direct comparison.\nThe TiTiler-Xarray service was then customized to work with the specific folder structure of the ZARR overviews previously created. Dockerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were deployed locally.\nPerformance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of six colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geospatial visualization context.\n\nSpeed up\nThe assessment of the performance of the new Data Catalog will be measured using the baseline scenario established at the beginning of the internship and the Speed Up metric proposed by [@durbha_advances_2023]:\n\\[ SpeedUp = \\frac{t_{baseline}}{t_{catalog}} \\]\nThis metric explains how much the process to access data has sped up thanks to the integration of cloud-based storage, the data catalog and the browsing interface."
  },
  {
    "objectID": "index.html#baseline-scenario",
    "href": "index.html#baseline-scenario",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\n\nMain findings\nThe main finding of the interviews were the steps followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 3 and show how complex and time consuming the process of discovering and visualizing spatial data can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying out.\n\n\n\n\n\n\nFigure 3: Baseline workflow\n\n\n\nThe major pitfalls found on the process of data discovery in the company could be summarized in:\n\nHigh dependency on colleagues.\nDisorganized structure of Google Storage Buckets.\nExperienced study subjects would find the data location faster.\nData discovery also dependent on recurrent work with a specific dataset (Google Console highlighted links).\nNot intuitive naming of repositories."
  },
  {
    "objectID": "index.html#service-integration",
    "href": "index.html#service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Service integration",
    "text": "Service integration\nExplain here how eoAPI uses multiple services, how each of them helps S11 in their data discovery and vizz tasks, and how did I manage to deploy it\nKubernetes\nSTAC-API, pgSTAC, TiTiler\n\nData discovery improvement\nFlowchart with STAC"
  },
  {
    "objectID": "index.html#performance-of-multi-format-data-visualization",
    "href": "index.html#performance-of-multi-format-data-visualization",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nTiTiler-PgSTAC & TiTiler-xarray\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented on Figure 4. In the figure it can be obesrved that COG tiles are requested 2.38 times faster than the same file in ZARR format.\n\n\n\n\n\n\n\n\nFigure 4: Request times depending on data format and zoom level\n\n\n\n\n\n\n\nEffects of zoom level\nAs seen on Figure 5, the zoom level of the map will have an effect on the time spent requesting and getting a tile from a tile server. In this study, it was found that the request times decreased by a factor of -0.013 and -0.005 per zoom level for COGs and ZARRs respectively.\n\n\n\n\n\n\n\n\nFigure 5: Request times depending on zoom level"
  },
  {
    "objectID": "index.html#code-to-evaluate-request-times",
    "href": "index.html#code-to-evaluate-request-times",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 9:\n        change_range = 3\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11 Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n\n        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/{tile}%401x?url=gs://s11-tiles/zarr/separate&variable=band_data&reference=false&decode_times=true&consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"http://localhost:8082/collections/Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/WebMercatorQuad/{tile}%401x?bidx=1&assets=data&unscale=false&resampling=nearest&reproject=nearest&colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter.csv')"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSTAC extensions are additional metadata properties that can be added to a dataset. (e.g. Classes, bands, sensor-type, etc.)↩︎"
  },
  {
    "objectID": "sections/discussion.html",
    "href": "sections/discussion.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "hgdcjfc rah fdxh sdfhfn dfayhgn d\ngadfh"
  },
  {
    "objectID": "sections/results.html",
    "href": "sections/results.html",
    "title": "Baseline scenario",
    "section": "",
    "text": "The main finding of the interviews were the steps followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 1 and show how complex and time consuming the process of discovering and visualizing spatial data can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying out.\n\n\n\n\n\n\nFigure 1: Baseline workflow\n\n\n\nThe major pitfalls found on the process of data discovery in the company could be summarized in:\n\nHigh dependency on colleagues.\nDisorganized structure of Google Storage Buckets.\nExperienced study subjects would find the data location faster.\nData discovery also dependent on recurrent work with a specific dataset (Google Console highlighted links).\nNot intuitive naming of repositories."
  },
  {
    "objectID": "sections/results.html#service-integration",
    "href": "sections/results.html#service-integration",
    "title": "Baseline scenario",
    "section": "Service integration",
    "text": "Service integration\nExplain here how eoAPI uses multiple services, how each of them helps S11 in their data discovery and vizz tasks, and how did I manage to deploy it\nKubernetes\nSTAC-API, pgSTAC, TiTiler\n\nData discovery improvement\nFlowchart with STAC"
  },
  {
    "objectID": "sections/results.html#performance-of-multi-format-data-visualization",
    "href": "sections/results.html#performance-of-multi-format-data-visualization",
    "title": "Baseline scenario",
    "section": "Performance of multi-format data visualization",
    "text": "Performance of multi-format data visualization\nTiTiler-PgSTAC & TiTiler-xarray\n\nRaster formats\nThe comparison of visualization speeds with TiTiler-xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented on Figure 2. In the figure it can be obesrved that COG tiles are requested 2.38 times faster than the same file in ZARR format.\n\n\n\n\n\n\n\n\nFigure 2: Request times depending on data format and zoom level\n\n\n\n\n\n\n\nEffects of zoom level\nAs seen on Figure 3, the zoom level of the map will have an effect on the time spent requesting and getting a tile from a tile server. In this study, it was found that the request times decreased by a factor of -0.013 and -0.005 per zoom level for COGs and ZARRs respectively.\n\n\n\n\n\n\n\n\nFigure 3: Request times depending on zoom level"
  },
  {
    "objectID": "sections/methodology.html",
    "href": "sections/methodology.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "To address the proposed research questions, a series of activities and the deliverables expected from them are presented on Figure 1. Additionally, a detailed description of each activity is presented in the following subsections."
  },
  {
    "objectID": "sections/methodology.html#data-and-processing-code-familiarization",
    "href": "sections/methodology.html#data-and-processing-code-familiarization",
    "title": "S11-CATS Report",
    "section": "Data and processing code familiarization",
    "text": "Data and processing code familiarization\nIn this step, I will familiarize myself with the current tools used for the processing of the images and its storage. This step will include the understanding of cloud services and internal image processing tools and the main datasets to be referenced on the catalog (STAC data). Moreover, in this step an initial description of the STAC data metadata will be performed.\n\nCloud services familiarization\nAn overview of the cloud services used by the company will be described. This will be mainly with the objective to understand, but not limited to:\n\nWho access the data?\nWhat are the costs of accessing it?\nHow often certain data is updated?\nHow is the data updated?"
  },
  {
    "objectID": "sections/methodology.html#baseline-scenario-definition",
    "href": "sections/methodology.html#baseline-scenario-definition",
    "title": "S11-CATS Report",
    "section": "Baseline scenario definition",
    "text": "Baseline scenario definition\nThe baseline scenario was defined as the set of methods currently being used by members of different teams at Satelligence to find, retrieve and visualize spatial data. This baseline scenario was evaluated qualitatively by interviewing four members of two different teams in the company (i.e. The data and the operations team). To keep a balance regarding experience of the study subjects, both the newest member of each team and a member with at least three years in the company were interviewed.\nThe questions asked during the interviews were oriented towards two main topics that were covered during this internship: Spatial data discovery and spatial data visualization. For both topics, the questions were divided into questions related to raster and vector datasets. The questions included in the interview can be found in appendix ### and were meant to be open questions with multiple possible answers."
  },
  {
    "objectID": "sections/methodology.html#data-integration",
    "href": "sections/methodology.html#data-integration",
    "title": "S11-CATS Report",
    "section": "Data integration",
    "text": "Data integration\n\nLocal STAC creation and browsing\nThis step will mainly be focused on the set up of the developing environment to both create a local STAC catalog and browse through it. This will include:\n\nCreation of a virtual environment or Docker container with all the required packages to create a STAC catalog.\nCreation of Local STAC using sample data from the company.\nBrowse through the STAC catalog using tools like STAC browser.\n\nThe deliverable of this step will be a GitLab repository with code to create a catalog, add assets from a local directory and browse through them locally using STAC browser.\n\n\nOrganization of main STAC structure & extensions per asset\nIn this step the structure of the STAC catalog will be defined. This will involve the selection of datasets that will be referenced on the catalog, the definition of subcatalogs and/or collections to group items with similar metadata. An initial idea of the structure of the main STAC catalog can be seen on Figure 2. Initially, the creation of two different subcatalogs is proposed to keep the static and dynamic dataset separated. Moreover, the selection of the STAC extensions1 used for each dataset will be defined in this step.\n\n\n\n\n\n\nFigure 2: Initial proposed STAC structure\n\n\n\n\n\nBuild main STAC v.0.1\nThis step will focus on the building of the initial version of the main STAC catalog, once the datasets and the overall structure has been defined. It will involve the population of the catalog with STAC components following the defined structure on Section 3.2. Furthermote, on this step a series of validation tools will be used to check that the STAC catalog created is followins the STAC spcification. These tools are part of the python package stac-tools.\nThe deliverable of this step will be a GitLab repository with code to create a catalog, create collections, add assets from a directory on the cloud and update them.\n\n\nSet up STAC browser on GCP\nIn this step a version of the STAC Browser application will be deployed using the tools from Google Cloud Platform (GCP). This application will allow users to browse and interact with the STAC catalog through a user-friendly interface. Additionally, this step will involve the definition of resources and tools from GCP that will be employed to deploy the application. For instance, the decision of doing it through a virtual machine or on a containerized way will be made.\nThe deliverable of this step will be a the STAC browser application running on GCP.\n\n\nAutomate processes via CI pipeline\nFinally, the code to create, modify and/or deploy the STAC catalog will be merged into a continuous integration pipeline that will allow the integration of this catalog with other tools from the company. For instance, the Distributed Processing Framework (DPROF), which is satelligence’s Satellite Data Processing engine."
  },
  {
    "objectID": "sections/methodology.html#multi-format-data-visualization",
    "href": "sections/methodology.html#multi-format-data-visualization",
    "title": "S11-CATS Report",
    "section": "Multi-format data visualization",
    "text": "Multi-format data visualization\nTo assess the performance of dynamic tiling services for visualizing Cloud Optimized GeoTIFFs (COGs) and Zarr data formats, the following approach was undertaken. A COG containing forest baseline information for the Riau region of Indonesia was used to create a series of Zarr files, each representing different overviews corresponding to various zoom levels. This preprocessing step, completed by the company prior to the study, ensured that the same data was used across both data formats, allowing for direct comparison.\nThe TiTiler-Xarray service was then customized to work with the specific folder structure of the ZARR overviews previously created. Dockerized versions of both TiTiler-Xarray (for Zarr files) and TiTiler-PgSTAC (for COG files) were deployed locally.\nPerformance was measured by recording the response times for random tile requests at zoom levels ranging from 9 to 18. Finally, to mitigate the influence of cached data on response times, each iteration used a different colormap, with a total of six colormaps employed. This methodology enabled a systematic evaluation of the performance differences between the two data formats in a geospatial visualization context.\n\nSpeed up\nThe assessment of the performance of the new Data Catalog will be measured using the baseline scenario established at the beginning of the internship and the Speed Up metric proposed by [@durbha_advances_2023]:\n\\[ SpeedUp = \\frac{t_{baseline}}{t_{catalog}} \\]\nThis metric explains how much the process to access data has sped up thanks to the integration of cloud-based storage, the data catalog and the browsing interface."
  },
  {
    "objectID": "sections/methodology.html#footnotes",
    "href": "sections/methodology.html#footnotes",
    "title": "S11-CATS Report",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSTAC extensions are additional metadata properties that can be added to a dataset. (e.g. Classes, bands, sensor-type, etc.)↩︎"
  },
  {
    "objectID": "sections/appendix.html",
    "href": "sections/appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "import pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 9:\n        change_range = 3\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11 Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n\n        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/{tile}%401x?url=gs://s11-tiles/zarr/separate&variable=band_data&reference=false&decode_times=true&consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"http://localhost:8082/collections/Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/WebMercatorQuad/{tile}%401x?bidx=1&assets=data&unscale=false&resampling=nearest&reproject=nearest&colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter.csv')"
  },
  {
    "objectID": "sections/appendix.html#code-to-evaluate-request-times",
    "href": "sections/appendix.html#code-to-evaluate-request-times",
    "title": "Appendix",
    "section": "",
    "text": "import pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 9:\n        change_range = 3\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11 Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n\n        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/{tile}%401x?url=gs://s11-tiles/zarr/separate&variable=band_data&reference=false&decode_times=true&consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"http://localhost:8082/collections/Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/WebMercatorQuad/{tile}%401x?bidx=1&assets=data&unscale=false&resampling=nearest&reproject=nearest&colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter.csv')"
  },
  {
    "objectID": "sections/abstract.html",
    "href": "sections/abstract.html",
    "title": "Executive summary",
    "section": "",
    "text": "Executive summary\n\n\n\nList of abreviations\n\nAbbreviation list\n\n\n\n\n\n\nAbreviations\nDescription\n\n\n\n\nEUDR\nEuropean Union Deforestation Regulation\n\n\nSTAC\nSpatio-Temporal Asset Catalog\n\n\nCOG\nCloud-Optimized GeoTiff\n\n\nOGC\nOpen Geospatial Consortium\n\n\nSDI\nSpatial Data Infrastructure\n\n\nS11\nSatelligence\n\n\nK8\nKubernetes\n\n\nDPROF\nDistributed Processing Framework\n\n\nJSON\nJavaScript Object Notation\n\n\nAPI\nApplication Programming Interfaces\n\n\nHTTP\nHyperText Transfer Protocol\n\n\nSQL\nStandard Query Language"
  },
  {
    "objectID": "sections/introduction.html",
    "href": "sections/introduction.html",
    "title": "S11-CATS Report",
    "section": "",
    "text": "Satelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) [@satelligence_internship_2023]. S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "sections/introduction.html#internship-organization-background",
    "href": "sections/introduction.html#internship-organization-background",
    "title": "S11-CATS Report",
    "section": "",
    "text": "Satelligence (S11) is a company founded in 2016 that specializes in providing satellite-based actionable information by monitoring environmental risks in commodity supply chains and financial investment planning [@satelligence_home_nodate]. More specifically, the company processes terabytes of satellite imagery to detect environmental risks and presents this information to their clients in a web application to assist them in the migration towards more sustainable sourcing models and the compliance with deforestation-free commodities regulations, such as the European Union Deforestation Regulation (EUDR) [@satelligence_internship_2023]. S11’s main focus is continuous deforestation monitoring (CDM) in the tropics using freely accessible satellite imagery. This is a data-intensive task that is achieved by leveraging the benefits of cloud computing, specifically Google Cloud Platform."
  },
  {
    "objectID": "sections/introduction.html#context-and-justification-of-research",
    "href": "sections/introduction.html#context-and-justification-of-research",
    "title": "S11-CATS Report",
    "section": "Context and justification of research",
    "text": "Context and justification of research\nSatelligence strongly relies on cloud computing for their services. They process extensive volumes of satellite imagery amounting to terabytes using DPROF, a distributed processing framework created within the company to efficiently process multidimensional spatial datasets. While this processing workflow currently runs smoothly, the company’s data and operations teams face challenges when going deeper into the analysis and accessing intermediate results due to the big nature of this data [@satelligence_internship_2023]. Scholars have defined big data as datasets characterized by their high Volume, Velocity, and Variety, which makes it paramount to use advanced processing and analytics techniques to derive relevant insights [@giri_big_2014]. In the specific case of Satelligence, their datasets can be categorized as big data due to their: High volume (Terabytes of satellite images processed every day), high velocity (Near – real time processing of these images) and high variety (Imagery coming from different sensors and regions). All these datasets are a specific case of big data: Big Geodata.\n\nSignificance of the topic and previous research\nIn the past decades there has been a rapid increase in the amount and size of geo-spatial information that can be accessed. Nowadays, more than 150 satellites orbit the earth collecting thousands of images every single day [@zhao_scalable_2021]. This has made data handling and the introduction of spatial data infrastructures (SDIs) paramount when working with such big datasets.\nTraditionally, SDIs have served to ease the accessibility, integration and analysis of spatial data [@rajabifard_spatial_2001]. However, in practice SDIs have been built upon technologies that focus on data preservation rather than accessibility [@durbha_advances_2023]. Due to this, an important shift is underway towards more cloud-based SDIs [@tripathi_cloud_2020]. These platforms need the emergence of new technologies that prioritize seamless access to cloud-stored data, efficient discovery services that ensure the easy location of extensive spatial data, and data visualization interfaces where multiple datasets can be depicted.\n\nCloud-based data storage\nSpatial data, just like any other type of data, can be cataloged into structured and unstructured data. Structured datasets are often organized and follow a specific structure (i.e. A traditional table with rows (objects) and columns (features)). On the other hand, unstructured data does not have a predefined structure (e.g. Satellite imagery and Time series data) [@mishra_structured_2017]. The management of structured data has witnessed substantial advancements, making it straightforward to handle it systematically using, for instance, relational databases (i.e. With the help of Structured Query Language (SQL)) [@kaufmann_database_2023]. In contrast, due to the additional challenges associated with the handling of unstructured data, the developments in this area have taken a longer time to appear.\nThe emergence of cloud-based archives has been one of the main advancements for unstructured data management during the last decades. In the specific case of geo-spatial data, it has allowed to store terabytes of unstructured data (i.e. Satellite imagery) on the cloud and access it through the network. However, the necessity transmitting data across networks to access it makes it essential to develop new data formats suited for such purposes [@durbha_advances_2023].\nAt S11, the storage of large geo-spatial data is already managed using Google Storage Buckets, and they are currently in the process of incorporating the conversion to cloud-optimized data formats like Cloud Optimized GeoTIFFs (COGs) and Zarrs in their processing framework (DPROF) to improve efficiency and accessibility.\n\nCloud-optimized data formats\n\nCOG\nCloud-Optimized GeoTIFFs (COGs) are an example of data formats that have been created to ease the access of data stored in the cloud. They improve the readability by including the metadata in the initial bytes of the file stored, storing different image overviews for different scales and tiling the images in smaller blocks. These characteristics make COG files heavier than traditional image formats. However, they also greatly enhance accessibility by enabling the selective transfer of only the necessary tiles using HTTP GET requests [@desruisseaux_ogc_2021]. Additionally, this data format has been adopted as an Open Geospatial Consortium (OGC) standard. These standards are a set of guidelines and specifications created to facilitat data interoperability [@ogc_ogc_2023].\n\n\nZarr\nAnother cloud native data format that has gained popularity recently is Zarr. This data format and python library focuses on the cloud-optimization of n-dimensional arrays. Zarr, differently than COGs store the metadata separately from the data chunks using lightweight external JSON files [@durbha_advances_2023]. Additionally, this data format stores the N-dimensional arrays in smaller chunks that can be accessed more easily. While the storage of Zarr files in chunks facilitates more efficient data access, the absence of overviews hinders the visualization of this data in a web map service [@desruisseaux_ogc_2021]. Due to the increasing use of Zarr for geo-spatial purposes, the OGC endorsed Zarr V2 as a community standard. Nevertheless, efforts are still being made to have a geo-spatial Zarr standard adopted by OGC [@chester_ogc_2024].\n\n\n\n\nData discovery services\nA discovery service that recently has become widely used for the exploration of big geo-data is Spatio-Temporal Asset Catalog (STAC). Through the standardization of spatio-temporal metadata, STAC simplifies the management and discovery of big geo-data [@brodeur_geographic_2019]. This service works by organizing the data into catalogs, collections, items, and assets stored as lightweight JSON formats (See Table 1) [@durbha_advances_2023].\nMoreover, there are two types of STAC catalogs: static and dynamic. Static catalogs are pre-generated and stored as static JSON files on a cloud storage. Static catalogs follow sensible hierarchical relationships between STAC components and this feature makes it easy to be browsed and/or crawled by. Nevertheless, these catalogs cannot be queried. On the other hand, dynamic catalogs are generated as APIs that respond to queries dynamically. Notably, dynamic catalogs will show different views of the same catalog depending on queries which usually focus on the spatio-temporal aspect of the data [@noauthor_stac-specbest-practicesmd_nodate].\n\n\n\nTable 1: STAC components\n\n\n\n\n\n\n\n\n\nSTAC components\nDescription\n\n\n\n\nAssets\nAn asset can be any type of data with a spatial and a temporal component.\n\n\nItems\nAn item is a GeoJSON feature with some specifications like: Time, Link to the asset (e.g. Google bucket)\n\n\nCollections\nDefines a set of common fields to describe a group of Items that share properties and metadata\n\n\nCatalogs\nContains a list of STAC collections, items or can also contain child catalogs.\n\n\n\n\n\n\nIn the specific case of dynamic catalogs, the concept of STAC API is widely used. In general, an API is a set of rules and protocols that enables different software applications to communicate with each other. In the case of the STAC API, it provides endpoints for searching and retrieving geospatial data based on criteria such as location and time, delivering results in a standardized format that ensures compatibility with various tools and services in the geospatial community. Moreover, even though STAC API is not an OGC standard or a OGC community standard, the basic requests performed in a STAC API adheres to the OGC API-Features standards for querying by bounding box and time range, returning GeoJSON-formatted results that conform to both STAC and OGC specifications. Ultimately, compared to OGC API-Features, STAC API enhances functionality by providing additional features that users needed (e.g. cross-collection search, versioning).\n\n\nVisualization interfaces\n\nThe visualization of spatial data brings with it a series of challenges due to its big nature. Dynamic tiling libraries such as TiTiler have tackled multiple of these challenges by creating APIs that dynamically generate PNG/JPEG image tiles when requested without reading the entire source file into memory [@noauthor_titiler_nodate]. This feature optimizes rendering of images since PNG and JPEG image file formats are more easily transferred through the web.\nTiTiler supports various data structures including STAC (SpatioTemporal Asset Catalog), Cloud Optimized GeoTIFFs (COGs), and is currently working on adding support for Zarrs. For the first two the TiTiler PgSTAC specialized extension integrates with PostgreSQL to enhance STAC catalog querying capabilities. For the case of Zarrs, the TiTiler-Xarray extension is being developed to facilitate the handling of multidimensional data arrays.\n\n\n\nAdded value of this research\nThis research aims to identify efficient solutions for the company’s current challenges in discovering and visualizing large geospatial datasets by integrating cloud-optimized data formats, cloud services, STAC specifications, and dynamic tiling services. The outcomes of this research will: offer valuable insights into the existing data discovery challenges within the company, propose a methodology for integrating discovery and visualization services, and evaluate the effectiveness of dynamic tiling for various cloud-optimized data formats."
  },
  {
    "objectID": "sections/introduction.html#research-questions",
    "href": "sections/introduction.html#research-questions",
    "title": "S11-CATS Report",
    "section": "Research questions",
    "text": "Research questions\n\nWhat are the current challenges, practices, and user experiences related to data discovery and data visualization in the company?\nHow does the integration of cloud-optimized data formats, cloud services and SpatioTemporal Asset Catalog (STAC) specifications influence the process and experiences of discovering big spatial data?\nTo what extent do dynamic tiling services can perform in visualizing different cloud-optimized data formats?"
  },
  {
    "objectID": "FinalReport.html#sec-baseline-q",
    "href": "FinalReport.html#sec-baseline-q",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario questionnaire",
    "text": "Baseline scenario questionnaire\n\nRelated to data discovery\n\nI am working for Wilmar in South East Asia. Do you know what is the forest baseline that I should use and where can I find it?\nI have been checking the results of the Soy map we created. Do you know which DEM was used for it? And where can I find it?\nDo you know which DEM is used as the terrain mask when using Sentinel 1 data?\nI need to access the concessions data provided by Grepalma. Where can I find it?\n\n\n\nRelated to data visualization\n\nI am interested on getting an overview of where was the primary forest present in Colombia in 2007. Could you visualize a layer with this data for me?\nI need to visualize the concessions provided by fedepalma. Could you do it for me?"
  },
  {
    "objectID": "FinalReport.html#sec-request-code",
    "href": "FinalReport.html#sec-request-code",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Code to evaluate request times",
    "text": "Code to evaluate request times\n\nimport pandas as pd\nimport requests\nimport random\n\ntiles = [\"9/399/254\", \"10/800/505\", \"11/1603/1012\",  \"12/3209/2042\", \n\"13/6407/4075\", \"14/12817/8159\", \"15/25678/16271\", \"16/51268/32552\", \n\"17/102503/65134\", \"18/205062/130211\"]\n\n# Tiles are slightly modified to try to avoid getting cached tiles\ndef modify_tile(tile):\n    parts = tile.split('/')\n    z = int(parts[0])\n    x = int(parts[1])\n    y = int(parts[2])\n\n    # Determine the range of change based on the value of z\n    if z &lt;= 9:\n        change_range = 3\n    elif z &lt;= 12:\n        change_range = 5\n    elif z &lt;= 15:\n        change_range = 10\n    elif z &lt;= 18:\n        change_range = 50\n\n    # Apply the change to x and y\n    x_change = random.randint(-change_range, change_range)\n    y_change = random.randint(-change_range, change_range)\n\n    new_x = x + x_change\n    new_y = y + y_change\n\n    # Return the modified tile as a string\n    return f\"{z}/{new_x}/{new_y}\"\n\ntimes_zarr = []\ntimes_cog = []\nz_level = []\ncmap_picked = []\n\n# The colormaps picked can be either a customized one for S11\n# Forest baseline or greens_r\ncmap = [\"_name=greens&rescale=0,70\",\"_name=greens_r&rescale=0,70\",\n        \"_name=blues&rescale=0,90\", \"_name=blues_r&rescale=0,90\",\n        \"_name=reds&rescale=0,80\", \"_name=reds_r&rescale=0,80\"]\n\nfor i in range(len(cmap)):\n\n    mod_tiles = [modify_tile(tile) for tile in tiles]\n    k = i\n\n    for tile in mod_tiles:\n\n        url_zarr = f\"http://localhost:8084/tiles/WebMercatorQuad/\"+\\\n        \"{tile}%401x?url=gs://s11-tiles/zarr/separate&\"+\\\n        \"variable=band_data&reference=false&decode_times=true&\"+\\\n        \"consolidated=true&colormap{cmap[k]}&return_mask=true\"\n\n        url_cog = f\"http://localhost:8082/collections/\"+\\\n        \"Example%20FBL%20Riau/items/FBL_V5_2021_Riau_cog/tiles/\"+\\\n        \"WebMercatorQuad/{tile}%401x?bidx=1&assets=data&\"+\\\n        \"unscale=false&resampling=nearest&reproject=nearest&\"+\\\n        \"colormap{cmap[k]}&return_mask=true\"\n\n        x = requests.get(url_zarr)\n        times_zarr.append(x.elapsed.total_seconds())\n\n        x = requests.get(url_cog)\n        times_cog.append(x.elapsed.total_seconds())\n\n        z_level.append(int(tile.split('/')[0]))\n\n        cmap_picked.append(k)\n\ndata = pd.DataFrame([cmap_picked, z_level, times_cog, times_zarr]).T\ndata.columns = ['colormap','zoom level','COG', 'ZARR']\n\ndata.to_csv('request_time_results_6iter.csv')"
  },
  {
    "objectID": "FinalReport.html#data-and-service-integration",
    "href": "FinalReport.html#data-and-service-integration",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Data and service integration",
    "text": "Data and service integration\nBriefly describe what will be included in the section: Explanation of the datasets included in the catalog, the building of the catalog (s11-cats)\n\nDatasets included\n\n\nS11-cats repository\nDescription of modules\n\nProposed Catalog structure\nThe structure of the STAC catalog proposed can be seen on Figure 1.\nIn it a selection of datasets that will be referenced in the catalog is presented and a hierachical structure of\nand definition of subcatalogs and/or collections to group items with similar metadata. Initially, the creation of two different subcatalogs is proposed to keep the static and dynamic dataset separated. Moreover, the selection of the STAC extensions1 used for each dataset will be defined in this step.\n\n\n\n\n\n\nFigure 1: Initial proposed STAC structure\n\n\n\n\n\n\nPgSTAC\n\n\neoAPI\n\nUse of docker containers to run individual applications that can connect to each other.\n\n\n\nCI pipeline"
  },
  {
    "objectID": "FinalReport.html#baseline-scenario-1",
    "href": "FinalReport.html#baseline-scenario-1",
    "title": "Cataloguing and visualizing big Geodata",
    "section": "Baseline scenario",
    "text": "Baseline scenario\n\nMain findings\nThe main finding of the interviews were the steps followed currently to discover, retrieve and visualize data. These steps are summarized on Figure 2 and show how complex and time consuming the process of discovering and visualizing spatial data can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying out.\n\n\n\n\n\n\nFigure 2: Baseline workflow\n\n\n\nThe major pitfalls found on the process of data discovery in the company could be summarized in:\n\nHigh dependency on colleagues.\nDisorganized structure of Google Storage Buckets.\nExperienced study subjects would find the data location faster.\nData discovery also dependent on recurrent work with a specific dataset (Google Console highlighted links).\nNot intuitive naming of repositories.\nNot one place where all existing data can be found.\nBig need on going towards a SKI, that is able to provide datasets based on questions like the ones on the questionnaire"
  }
]