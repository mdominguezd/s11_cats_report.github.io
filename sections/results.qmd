## Baseline scenario

### Current workflow

One of the main findings of the interviews was the process followed currently to discover, retrieve and visualize data. These steps are summarized on @fig-baseline and show how complex and time consuming these tasks can be for a Satelligence employee nowadays. Moreover, the steps followed were categorized in four classes depending on how much time is generally spent carrying it out.

![Baseline workflow](img/Baseline_data_discovery_workflow.png){#fig-baseline width="100%"}

According to @fig-baseline, some of the most time-consuming tasks were searching for data on Google Cloud Storage and downloading it for visualization. Additionally, seeking advice from colleagues about the dataset's location added a major uncertainty to the time estimates, as responses varied from very quick to considerably delayed or non existent.

### Thematic Content Analysis

When asked about the recurrent patterns on the interviews undertaken to define the baseline scenario, ChatGPT found four main topics:

-   There is a high uncertainty on the location of datasets and a high dependency on colleagues to find them.
-   Multiple sources and locations of data.
-   Data familiarity helps users locate data more quickly.
-   Use of specific tools and methods for different datasets.

After some refinement and a deeper analysis of the interviews, the major pitfalls found on the process of data discovery and visualization in the company were summarized as follows:

-   High dependency on colleagues for dataset location.
-   Disorganized structure of Google Storage Buckets.
-   Data familiarity helps users locate data more quickly.
-   Locating data is dependent on recurrent work with a specific dataset.
-   Not intuitive naming of repositories with datasets.
-   Understanding of diverse tools to access different data is currently necessary.
-   Download of data is required in most cases to visualize it.
-   Not one place where all existing data can be found.

All of these pitfalls highlight the need for a simpler data discovery implementation, where data visualization can also be integrated seamlessly. Previous studies have found that key difficulties for earth observation data discovery include heterogeneous query interfaces, and use of diverse metadata models [@miranda_espinosa_reviewing_2020]. To address these challenges, the approach should enable easy access to datasets based on specific queries, ensuring that users can efficiently locate and utilize the data they need. By harmonizing metadata standards and query protocols, the process of data discovery can be greatly improved, making it more accessible and user-friendly.

## Service integration

The integration of the services deployed resulted in a version of STAC Browser including three different collections containing datasets related to the forest baseline created by the company, elevation data from third party organizations and a collection for the comparison of COG and Zarr data. The web application can be accessed in [https://eoapi.satelligence.com/browser](https://eoapi.satelligence.com/browser/?.language=en).

### Effective integration

The effective integration was not an easy task and involved multiple aspects, ranging from editing data formats to facilitate their visualization, transitioning from a static to a dynamic catalog, customizing APIs, and finalizing with the correct deployment of the services.

#### Data formats {.unnumbered}

An essential step related to data formats was the edition of Zarr datasets to achieve their optimal visualization. This edition involved creating a series of overviews of the same dataset at different resolutions. Specifically, this was accomplished by converting COGs into multiple Zarr files resampled at various spatial resolutions. These resampled Zarr files, acting as overviews enhance visualization by allowing the appropriate resolution to be accessed based on the map scale, similar to the approach used when visualizing COGs [@lynnes_cloud_2020]. Even though the approach followed in this study allowed for improved visualization of Zarr files, the creation of Zarr pyramids in a more optimized way is still necessary. Other researchers have been focusing their efforts on this task to enhance the efficiency and effectiveness of the process [@Barciauskas_NextGen_2024].

#### Leverage of APIs {.unnumbered}

To effectively query the datasets stored in the catalog, a transition from a static to a dynamic catalog (i.e. a STAC API) was needed. This shift was facilitated by the deployment of a STAC API within the eoAPI framework. The STAC API facilitated the querying capabilities of the datasets stored in the catalog by dynamically requesting datasets based on their metadata. This dynamic setup not only facilitated data discovery but also enabled the use of additional tools such as the STAC API QGIS plugin. The plugin could simplify the process of data discovery and its direct manipulation.

For the visualization of Zarr datasets, it was necessary to customize the TiTiler-Xarray API to accommodate the new Zarr pyramid structure. This customization involved overwriting a series of functions in the main code of the application to align with the newly created Zarr pyramids. By adapting the API to handle the specific requirements of the Zarr format and its multi-resolution overviews, the visualization process was optimized.

#### Deployment {.unnumbered}

As described on @sec-eoapi, the deployment of both eoAPI and the additional services utilized was perform using Google Kubernetes Engine (GKE), which is K8s' GCP service. In a GKE cluster, the setup of complex multi-service applications that connect to each other with an internal network is simplified [@gupta_deployment_2021]. Moreover, [eoAPI](www.eoapi.dev) simplified the deployment by providing a guide for deployment that used a Helm chart. A Helm chart is a collection of files that describe the K8s related resources needed to run a multi-service application and it can improve the speed of deployment by a factor of up to 6 times [@gokhale_creating_2021]. These factors greatly influenced the decision of deploying the whole suite of services in eoAPI.

Moreover, the performance of some of the eoAPI services deployed using K8s had been already assessed by previous studies. For instance, @munteanu_performance_2024 performed tests on a deployed version of STAC API that, like the deployment performed in this study, used PgSTAC as the backend. These authors deployed a dynamic STAC API loaded with the metadata of approximately 2.3 TB of spatial data on a K8s cluster and evaluated the performance by assessing both the response times and resources used in a hypothetical scenario where 7,000 users would perform requests simultaneously. Their results showed that these services are capable of supporting effectively a much larger amount of users than the estimated by Satelligence.

Finally, additional to the already covered advantages, the deployment of [eoAPI](www.eoapi.dev) and the fast community driven development of new tools brings with it benefits that could become very important for S11's workflow. For instance, the visualization of vector data using [TiPg](https://github.com/developmentseed/tipg) is a service included in the eoAPI deployment that wasn't used during this internship, but should certainly be integrated in the near future by the company to visualize their supply chain datasets. Moroever, the community adopting STAC specifications has been growing fast. Due to this, a big series of STAC-extensions have been developed to fulfill the requirements of the users. During this internship, extensions to add additional metadata were included, however, due to time constraints other extensions that could prove beneficial for the company were not integrated. Specifically, the possibility of adding an authentication layer to the catalog still needs to be done and should be the next step to ensure the privacy of the data.

### Workflow improvement

Once the deployment of eoAPI and the extra services was done, a new workflow for both the new data discovery and visualization tasks was designed and is presented on @fig-new-workflow. This new workflow shows a clear improvement on the speed and the ease of use of the new methods employed.

![New data discovery and visualization workflow](img/New_data_discovery_workflow.png){#fig-new-workflow width="100%"}

Moreover, it can be seen that with the new implementation most of the issues identified on the TCA were addressed. There is no longer a dependency on colleagues for locating datasets, as all data is now consolidated in one place. The disorganized structure of Google Storage Buckets is no longer a concern since the catalog can integrate data stored in multiple buckets into a single, cohesive STAC collection. The previous issue of non-intuitive naming conventions for data repositories, is resolved because it is unnecessary to know the data source once it is included in the STAC catalog. Furthermore, there is no longer a need to understand diverse tools for accessing different data; the STAC Browser facilitates querying collections and visualizing items. Finally, the STAC catalog serves as the centralized location for all data used in S11 workflows, which favors long term usability of code that relies on this data.

## Performance of multi-format data visualization

The results of the experiments made with different cloud-optimized data formats are presented in two subsections. The first subsection evaluates the overall performance of the two data formats and the second subsection assess the performance of these data formats based on different zoom levels.

### Raster formats

```{python}
#| echo: false

import pandas as pd

data = pd.read_csv('https://raw.githubusercontent.com/mdominguezd/s11_cats_report.github.io/main/sections/request_time_results_6iter.csv')

speed_up = float(round(data['ZARR'].mean()/data['COG'].mean(), 2))
```

The comparison of visualization speeds with TiTiler-Xarray for Zarr datasets and TiTiler-PgSTAC for COGs are presented on  and @fig-format-comp. In the figure it can be observed that on average the response for requests of COG tiles was `{python} speed_up` times faster than the one for the same file in ZARR format. Moreover, @fig-format-comp shows that the response times for tiles created from data stored as Zarr showed a considerable wider range than the ones generated from data in the COG format, which indicates more variability in the performance for Zarr.

```{python}
#| echo: false
import folium
from folium.plugins import DualMap

# Example URL template for the tiles
tiles_url_cog = "https://eoapi.satelligence.com/raster/collections/Example ZARR vs. COG/items/FBL_V5_2021_Riau_cog/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?bidx=1&assets=data&unscale=false&resampling=nearest&reproject=nearest&colormap=[[[1.0,1.1],[0,47,0,255]],[[2.0,2.1],[55,76,33,255]],[[3.0,3.1],[105,140,60,255]],[[4.0,4.1],[178,199,140,255]],[[5.0,5.1],[164,198,121,255]],[[6.0,6.1],[198,112,85,255]],[[7.0,7.1],[170,219,167,255]],[[8.0,8.1],[87,162,164,255]],[[50.0,50.1],[255,183,1,255]],[[52.0,52.1],[238,223,201,255]],[[53.0,53.1],[185,120,119,255]],[[54.0,54.1],[218,170,241,255]],[[55.0,55.1],[40,205,167,255]],[[60.0,60.1],[208,227,243,255]],[[66.0,66.1],[166,219,204,255]],[[70.0,70.1],[255,255,255,255]],[[71.0,71.1],[185,136,94,255]],[[72.0,72.1],[125,165,142,255]],[[74.0,74.1],[188,85,123,255]],[[90.0,90.1],[241,195,132,255]]]&return_mask=true"

tiles_url_zarr = "https://eoapi.satelligence.com/zarr/tiles/WebMercatorQuad/{z}/{x}/{y}@1x?url=gs://s11-tiles/zarr/separate&variable=band_data&reference=false&decode_times=true&consolidated=true&colormap=[[[1.0,1.1],[0,47,0,255]],[[2.0,2.1],[55,76,33,255]],[[3.0,3.1],[105,140,60,255]],[[4.0,4.1],[178,199,140,255]],[[5.0,5.1],[164,198,121,255]],[[6.0,6.1],[198,112,85,255]],[[7.0,7.1],[170,219,167,255]],[[8.0,8.1],[87,162,164,255]],[[50.0,50.1],[255,183,1,255]],[[52.0,52.1],[238,223,201,255]],[[53.0,53.1],[185,120,119,255]],[[54.0,54.1],[218,170,241,255]],[[55.0,55.1],[40,205,167,255]],[[60.0,60.1],[208,227,243,255]],[[66.0,66.1],[166,219,204,255]],[[70.0,70.1],[255,255,255,255]],[[71.0,71.1],[185,136,94,255]],[[72.0,72.1],[125,165,142,255]],[[74.0,74.1],[188,85,123,255]],[[90.0,90.1],[241,195,132,255]]]&return_mask=true"

# Create a folium map centered at a specific latitude and longitude
m = DualMap(location=[0.596093307985066, 101.79719686886921], zoom_start=8)

# Add the tile layer to the map
folium.TileLayer(
    tiles=tiles_url_cog,
    attr='Example Tile Server',
    name='COG tiles',
    overlay=True,
    control=True
).add_to(m.m1)

folium.TileLayer(
    tiles=tiles_url_zarr,
    attr='Example Tile Server',
    name='Zarr tiles',
    overlay=True,
    control=True
).add_to(m.m2)


# Add layer control to toggle the tile layer
folium.LayerControl().add_to(m.m1)
folium.LayerControl().add_to(m.m2)

m.save('format_comp.html')

m
```

<iframe src="https://raw.githubusercontent.com/mdominguezd/s11_cats_report.github.io/main/sections/format_comp.html" style="height:100%;width:100%;"  title="martindominguezduran"></iframe>

```{python}
#| echo: false
#| fig-cap: "Response times for tile requests depending on data format and zoom level"
#| label: fig-format-comp

import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import xarray as xr


s11_cmap = LinearSegmentedColormap.from_list('s11', ['#96d7fa', '#ff4100'])
background_color = '#180066'
text_color = 'white'

sns.set(rc={'axes.facecolor' : 'white',
    'grid.color' : 'lightgray',
    'text.color' : background_color,
    'axes.labelcolor' : background_color,
    'xtick.color': background_color,
    'ytick.color': background_color,})

fig, ax = plt.subplots(1,1,figsize= (7.5,5))

sns.boxplot(data[['COG', 'ZARR']], palette = {'COG':s11_cmap(0), 'ZARR':s11_cmap(255)}, ax = ax, linecolor=background_color)
# sns.despine(trim = True, offset = -10, ax = ax)

ax.set_ylabel('Response time [s]')
a = ax.set_xlabel('Data format')

```

The results obtained are coherent to the ones previously shown by @nasa_impact_zarr_2023, where COGs' rendering time was found to be lower than the one for Zarr files at different zoom levels. These results show that COGs, being specifically optimized for spatial data visualization, offer faster visualization compared to Zarr files. However, this does not take away from the fact that Zarr provides additional benefits, such as the ability to store n-dimensional arrays. Furthermore, recent advancements like GeoZarr and the creation of Zarr pyramids with new packages like [ndpyramid](https://github.com/carbonplan/ndpyramid) could bring significant improvements to this data format [@pagan_current_2023, @Barciauskas_NextGen_2024].

#### Fine tuning of dataset {.unnumbered}

Even though COGs showed to be very performant, their performance could be further enhanced by tuning specific GDAL parameters. These adjustments could improve the speed of tiling services using COG [@nasa_impact_zarr_2023]. Future considerations should include optimizing these parameters to maximize efficiency (See [performance tuning section](https://developmentseed.org/titiler/advanced/performance_tuning/)).

### Effects of zoom level

```{python}
#| echo: false

import statsmodels.api as sm
data = pd.read_csv('https://raw.githubusercontent.com/mdominguezd/s11_cats_report.github.io/main/sections/request_time_results_6iter.csv')


fit_results = sm.OLS(data['COG'], sm.add_constant(data['zoom level'])).fit()
cog_slope = float(round(fit_results.params['zoom level'], 3))

fit_results = sm.OLS(data['ZARR'], sm.add_constant(data['zoom level'])).fit()
zarr_slope = float(round(fit_results.params['zoom level'], 3))

```

As seen on @fig-comp-zoom, the zoom level of the map showed an effect on the time spent requesting and getting a tile from a tiling service for the COG format. In this study, it was found that the request times decreased by `{python} cog_slope` seconds per zoom level for COGs, and didn't show a notable change for Zarrs (`{python} zarr_slope` seconds per zoom level). The behavior presented here for COGs differs from the one observed by @nasa_impact_zarr_2023, where no difference in rendering time was observed as a function of the zoom level. This difference could be explained by the fact that in their study, only the lowest zoom levels were considered, while in this study only the highest zoom levels were taken into account, however, to verify this hypothesis a broader study of the response times at more zoom levels should be performed. This was not done in this study because the limited size of the study area that the raster images covered only allowed visualization of the data at high zoom levels (i.e., above 8).

```{python}
#| echo: false
#| fig-cap: "Request times depending on zoom level and their respective trends for COG and Zarr data formats."
#| label: fig-comp-zoom
fig, ax = plt.subplots(1,1,figsize= (7.5,3.5))

sns.regplot(data, x = 'zoom level', y = 'COG', ax = ax, color=s11_cmap(0), ci = 95, label='COG')
sns.regplot(data, x = 'zoom level', y = 'ZARR', ax = ax, color=s11_cmap(255), ci = 95, label= 'ZARR')
ax.set_ylabel('Response time [s]')

a = plt.legend()
# a= sns.despine(trim = True, offset = -10, ax = ax)
```

Moreover, as seen on @fig-chunksize-zarr, while the size of blocks remain constant throughout all of the overviews in the COG file, the sizes of the Zarr chunks varied in the pyramids created. Due to this, the tiles requested at higher zoom levels were larger than the ones requested at lower zoom levels which could explain the difference between the trends observed in @fig-comp-zoom for the two data formats.

```{python}
#| echo: false
#| eval: false

import xarray as xr
import google.auth

google.auth.default()

chunk_x = []
chunk_y = []
pyramids = range(9)

for i in pyramids:

    zarr_link = 'gs://s11-tiles/zarr/separate'

    zarr_link = f"{zarr_link}/" + str(int(i))

    with xr.open_zarr(zarr_link, chunks = 'auto', consolidated = True, decode_coords='all') as src:
        
        # print(len(src['band_data'].chunksizes['y']), len(src['band_data'].chunksizes['x']))
        chunksize_y = src['band_data'].chunksizes['y'][0]
        chunksize_x = src['band_data'].chunksizes['x'][0]
        
        chunk_x.append(chunksize_x)
        chunk_y.append(chunksize_y)

df = pd.DataFrame([pyramids, chunk_x, chunk_y]).T
df.columns = ['pyramids', 'chunksize_x', 'chunksize_y']

df.to_csv('chunksize_zarr.csv')
```

```{python}
#| echo: false 
#| fig-cap: "Variation of chunk sizes in Zarr file depending on zoom level compared to a constant block size for COG data format."
#| label: fig-chunksize-zarr

df = pd.read_csv('https://raw.githubusercontent.com/mdominguezd/s11_cats_report.github.io/main/sections/chunksize_zarr.csv')

fig, ax = plt.subplots(1,1,figsize= (7.5,3.5))

ax = sns.scatterplot(df, x = 'pyramids', y = 'chunksize_x', color=s11_cmap(255), ax = ax, label = 'Horizontal chunks (x)')
ax = sns.scatterplot(df, x = 'pyramids', y = 'chunksize_y', color=s11_cmap(0), ax = ax, label = 'Vertical chunks (y)')

ax.axhline(y = 512)
ax.text(s = 'COG blocksize', x = 0, y = 512)

ax.set_ylabel('Chunk size [pixels]')
a = ax.set_xlabel('Pyramid\n' + r'$\rightarrow{}$zoom increase$\rightarrow{}$')

```